{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The hidden dimension of the Transformer model.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads in each layer. This parameter is crucial for determining the size and structure of multi-head self-attention mechanisms within Transformers.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] Dimensionality used by rotary position embeddings to encode positional information into queries and keys. This parameter affects how attention weights are calculated based on relative positions between tokens in the sequence.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] The dimension of each head within multi-head self-attention mechanisms, which is derived from `dim` and `num_heads`. It affects tensor shapes for QKV projections and output dimensions in attention layers.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence length of the input sequence. This parameter determines the size along the sequence dimension (second axis) of tensors like queries, keys, values, and output in attention mechanisms.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] Batch size for processing multiple sequences simultaneously during training or inference. It affects tensor shapes across different layers including input embeddings, QKV projections, and final outputs."
}

