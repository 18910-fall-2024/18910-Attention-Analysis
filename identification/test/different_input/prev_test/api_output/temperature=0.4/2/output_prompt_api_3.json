{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Sequence length for each sequence in the batch.",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Number of attention heads used by the model.",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Dimensionality (width) of each head in multi-head self-attention layer.",
    "dropout_p": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Probability for dropout used during training to prevent overfitting by randomly setting some activations to zero.",
    "max_seqlen_q": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Maximum sequence length in the query sequences of a batch. Used as input size constraint when calling `varlen_fwd` function from CUDA code.",
    "max_seqlen_k": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Maximum sequence length in the key-value pairs for attention computation, used to determine memory allocation and indexing boundaries within batches of sequences."
}

