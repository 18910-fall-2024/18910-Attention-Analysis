{
    "embed_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The dimension of the embedding, which is also the hidden size for each layer in a Transformer model.",
    "num_heads": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of attention heads used by the multi-head self-attention mechanism. Determines how many parallel, smaller matrices are created from the input matrix for computation efficiency and effectiveness.",
    "head_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The dimensionality (size) of each head in a multi-headed attention layer; it is calculated as embed_dim divided by num_heads. This parameter determines how much information can be processed per attention head.",
    "rotary_emb_dim": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The dimension of the rotary position embedding, which is used to incorporate positional information into self-attention. It must divide embed_dim evenly and be divisible by 16.",
    "softmax_scale": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The scaling factor applied before computing the softmax in attention scores, typically set to either a default value or sqrt(head_dim). It helps stabilize gradients and numerical precision during training.",
    "causal": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] A boolean indicating whether the model should enforce causal attention, meaning that each token can only attend to tokens before it. This is crucial for autoregressive models like language modeling.",
    "window_size": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] A tuple defining the left and right window sizes used in local (sliding) attention, which restricts each token to attend only within a sliding window around it. If set as (-1,-1), no such restriction is applied.",
    "rotary_interleaved": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] A boolean indicating whether the rotary embeddings should be interleaved, meaning that each dimension in the embedding corresponds to half of a sinusoidal position encoding. This is used for models like GPT-NeoX.",
    "num_splits": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The number of splits along sequence length dimension when processing key and value matrices, which can help in optimizing memory usage during training or inference."
}

