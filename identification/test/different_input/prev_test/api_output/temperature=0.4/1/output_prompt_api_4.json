{
    "seqlen": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibi.flash_rotary] The sequence length of the input tensor. Determines the size and shape of rotary embeddings.",
    "rotary_dim": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin] Dimensionality for applying Rotary Position Embedding to query (Q) and key (K).",
    "batch_size": "[flash_attn.flash_attn_interface.flash_attn_func, benchmarks.benchmark_alibi.time_fwd_bwd] The batch size of the input tensors. Determines the first dimension of Q, K, V.",
    "seqlen_q": "[benchmarks.benchmark_alibi.attn_bias_from_alibi_slopes, flash_attn.flash_attn_interface._flash_attn_forward] Sequence length for queries in attention mechanism. Used to determine relative position bias and shapes of tensors involved in causal masks or alibi slopes.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_func, benchmarks.benchmark_alibi.time_fwd_bwd] Boolean indicating whether the model should use a causal mask (True) for auto-regressive modeling. Determines if attention mechanism is unidirectional based on sequence length and shapes of tensors involved.",
    "nheads": "[flash_attn.flash_attn_interface._flash_attn_forward, benchmarks.benchmark_alibi.time_fwd_bwd] Number of heads in multi-head self-attention layer. Used to determine the second dimension (head number) for Q, K, V input dimensions.",
    "headdim": "[benchmarks.benchmark_alibi.flops, flash_attn.flash_attn_interface._flash_attn_forward] Dimensionality per head within a multi-headed attention mechanism. Determines third and fourth dimensions of Q, K, V tensors (head dimension).",
    "dropout_p": "[flash_attn.flash_attn_interface.flash_attn_func, benchmarks.benchmark_alibi.time_fwd_bwd] Dropout probability for the softmax output during training to prevent overfitting.",
    "softmax_scale": "[flash_attn.flash_attn_interface._flash_attn_forward, flash_attn.flash_attn_interface.FlashAttnFunc.forward] Scaling factor applied before computing attention scores. Used in scaling QK^T matrix product.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_func, benchmarks.benchmark_alibi.time_fwd_bwd] Tuple indicating the left and right window size for sliding local attention mechanism (if not -1). Determines how far each query can attend to keys in a sequence."
}

