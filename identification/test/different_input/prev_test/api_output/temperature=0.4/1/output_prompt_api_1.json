{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the embedding or hidden states in Transformer models.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used for multi-head self-attention mechanism. It determines how many parallel attention processes will be run and combined to form a larger output tensor.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality (i.e., size) of the key, query, value projections per head in each multi-head self-attention layer. It is calculated as `embed_dim // num_heads` within MHA's initialization function.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality for rotary positional embeddings used to enhance the attention mechanism by incorporating relative positions between tokens in a sequence, especially useful when dealing with long sequences or transformers that need better handling of position information beyond simple absolute offsets. It is typically set as half of `head_dim`.",
    "seqlen": "[flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention] The length (or number) of sequence tokens in the input data, which affects how attention scores are computed and cached for efficient processing during both training and inference phases. It is used to determine when rotary embeddings should be updated.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] Number of sequences or samples processed at once through a batched operation such as forward pass in the Transformer model, impacting parallelism efficiency and memory usage."
}

