{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Sequence length for each sequence in the batch. It determines the size and shape of input tensors like qkv, cu_seqlens, etc., which are passed to `varlen_fwd`.",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Number of attention heads. It affects the shape and dimensionality of tensors like qkv (batch_size, seqlen, 3, nheads, headdim).",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Dimension size for each attention head. It affects the shape and dimensionality of tensors like qkv (batch_size, seqlen, 3, nheads, headdim).",
    "dropout_p": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dropout probability for the attention mechanism. It is used in `varlen_fwd` to control dropout during training.",
    "softmax_scale": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Scaling factor applied before computing softmax over QK^T matrix, typically set as 1/sqrt(headdim).",
    "causal": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Boolean indicating whether to apply causal attention mask. It affects the computation of QK^T matrix and softmax.",
    "window_size": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Tuple (left, right) defining sliding window local attention range for each query position."
}

