{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embeddings for each token in a sequence.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used by the Transformer model. This parameter determines how many parallel self-attention mechanisms are applied to the input data, and it is crucial for determining the dimensions of Q (queries), K (keys), V (values) matrices.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each attention head. It's calculated as `embed_dim / num_heads` in MHA class initialization and used to determine tensor shapes for queries, keys, values during the forward pass.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of rotary embeddings applied on Q (queries) and K (keys). It influences how positional information is encoded into attention computations through sinusoidal functions. This parameter must be divisible by 16 for interleaved embedding.",
    "seqlen": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention] The sequence length of the input data (tokens). It is used to determine tensor shapes and sizes in attention mechanisms. In `MHA`, it's passed as an argument for rotary embedding updates.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The number of sequences (or samples) processed at once during training or inference. It affects the batch dimension in tensor operations and memory allocation.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied to QK^T before applying softmax, which is crucial for numerical stability during attention computation. It can be set explicitly or defaults based on the head dimension.",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] A tuple defining a sliding window of local context in which each token attends to other tokens (left, right). It influences how far back and forward attention can reach.",
    "cache_seqlens": "[flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The sequence lengths of the cached keys/values. This parameter is used to determine where new key/value pairs should be inserted into existing caches.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] A boolean indicating whether rotary embeddings are interleaved (True) or not. This affects how sinusoidal positional encodings are applied to Q and K.",
    "num_splits": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] The number of splits along the sequence dimension for key/value tensors, which can be used as a heuristic optimization parameter."
}

