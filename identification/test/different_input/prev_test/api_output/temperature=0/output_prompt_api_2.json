{
    "embed_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The dimension of the embedding space, which is also referred to as hidden size in Transformers.",
    "num_heads": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.modules.mha.ParallelMHA.forward] Number of attention heads used by Multi-Head Attention (MHA).",
    "head_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The dimensionality of each head in the multi-head self-attention mechanism.",
    "rotary_emb_dim": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Dimension used for rotary position embeddings, which is half of `head_dim` if interleaved and full dimension otherwise. It affects the shape of positional embedding tensors.",
    "seqlen": "[flash_attn.modules.mha.ParallelMHA.forward] Sequence length of input tokens or queries in a batch.",
    "batch_size": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of sequences processed simultaneously. It affects the shape and size of tensors like Q (queries), K (keys), V (values).",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied to query-key dot products before computing softmax in attention mechanism.",
    "causal": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean indicating whether the model should use causal masking, which is crucial for autoregressive models like language modeling. It affects how queries attend to keys and values.",
    "window_size": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tuple indicating the left and right window size in local attention mechanism, which restricts each query's interaction with a fixed-size neighborhood of key-value pairs. It affects how queries attend to keys.",
    "rotary_interleaved": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean indicating whether rotary embeddings are interleaved, affecting the way positional information is applied in multi-head attention. It affects how queries and keys interact with position encodings.",
    "num_splits": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of splits along sequence dimension for key-value tensors, which can affect performance optimizations in the attention mechanism."
}

