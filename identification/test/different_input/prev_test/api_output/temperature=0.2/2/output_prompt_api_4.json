{
    "seqlen": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibi.flash_rotary.flash_attn_func] The sequence length of the input tensor. It determines the size and shape of tensors such as query (q), key (k), value (v) in attention mechanism.",
    "headdim": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibi.flash_rotary.flash_attn_func] The dimensionality of each head within the multi-head self-attention layer. It affects the shape and size of q, k, v tensors in attention mechanism.",
    "batch_size": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibi.flash_rotary.flash_attn_func] The batch size used for training or inference with Transformers. Determines the first dimension (batch) of input tensors such as q, k, v in attention mechanism.",
    "causal": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibri.flash_rotary.flash_attn_func] A boolean indicating whether to use causal masking for self-attention. It affects the shape and size of tensors by applying a mask that prevents positions from attending to future tokens.",
    "nheads": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibri.flash_rotary.flash_attn_func] The number of attention heads in multi-head self-attention. It determines the third dimension (head) of input tensors such as q, k, v and affects their size.",
    "dropout_p": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibri.flash_rotary.flash_attn_func] The dropout probability for attention mechanism during training. It does not affect the shape or dimensions of input tensors but is a hyperparameter used in Transformers' self-attention.",
    "softmax_scale": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibri.flash_rotary.flash_attn_func] The scaling factor for QK^T before applying softmax. It does not affect the shape or dimensions of input tensors but is a hyperparameter used in Transformers' self-attention.",
    "window_size": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibri.flash_rotary.flash_attn_func] The size of sliding window for local attention. It affects how the shape and context are handled within tensors but does not directly change their dimensions."
}

