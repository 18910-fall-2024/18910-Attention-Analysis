{
    "embed_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The embedding dimension of the Transformer model, which determines the size and shape of input tensors.",
    "num_heads": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of attention heads in each layer. This parameter influences both the structure of the multi-head self-attention mechanism as well as tensor dimensions and shapes for input tensors.",
    "head_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The dimensionality (size) of individual head's output, which is derived from embed_dim divided by num_heads. It affects the shape of QKV projection outputs in multi-head attention mechanism.",
    "rotary_emb_dim": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The rotary embedding dimension, which is used for applying positional embeddings to queries and keys. It impacts the shape of QKV tensors when using rotary position encoding.",
    "seqlen": "[flash_attn.modules.mha.ParallelMHA.forward] Sequence length (length of input sequence) that affects tensor shapes in multi-head attention mechanism during both training and inference phases.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Scaling factor for the QK^T product before applying softmax. This parameter influences how scores are computed in attention mechanism but does not directly affect tensor dimensions or shapes.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Whether to apply causal mask (for auto-regressive modeling). This parameter influences the structure of attention mechanism but does not directly affect tensor dimensions or shapes.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Sliding window size for local (sliding window) attention. This parameter influences the structure of attention mechanism but does not directly affect tensor dimensions or shapes.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Whether to apply rotary embedding in interleaved manner. This parameter influences the structure of attention mechanism but does not directly affect tensor dimensions or shapes.",
    "alibi_slopes": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Bias slopes for ALiBi (Attention with Linear Biases) to be added during attention computation. This parameter influences the structure of attention mechanism but does not directly affect tensor dimensions or shapes.",
    "num_splits": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Number of splits along sequence dimension for key and value tensors. This parameter influences how input tensors are processed but does not directly affect tensor dimensions or shapes."
}

