{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the embedding or hidden state in a Transformer model.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used for multi-head self-attention mechanism. It determines how many parallel attention computations are performed and affects both memory requirements and computational efficiency.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality (depth) of each head in the multi-headed attention layer, calculated as `embed_dim / num_heads`.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The dimension of rotary positional embeddings used to encode relative positions between tokens in the sequence. It must be divisible by 16 for interleaved embedding.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The length of a single input sequence in the batch. It determines how many tokens are processed at once and affects memory requirements.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] Number of sequences (or samples) being processed simultaneously, which impacts parallelism and resource allocation during training or inference.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Scaling factor applied to the dot product between query and key vectors before computing attention scores. It is often set as `1 / sqrt(head_dim)` for normalization.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Tuple defining left and right context window sizes in local (sliding-window) attention mechanism. If both values are -1, it indicates global or full-context attention.",
    "cache_seqlens": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tensor indicating the sequence lengths of cached keys/values for each batch element when using incremental decoding with KV caching."
}

