{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embeddings for each token in a sequence.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used by Multi-Head Attention (MHA).",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head in the multi-head self-attention mechanism.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] Dimension for rotary position embeddings used to modify query and key vectors before computing attention scores. This is typically half of the head dimension.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.forward, tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] The length or sequence length for input sequences in Transformer models.",
    "batch_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, tests.modules.test_block_parallel.test_block_parallel] Number of samples (sequences) processed at once during training/inference. This is the batch size used when processing inputs through a model.",
    "softmax_scale": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied to QK^T before applying softmax in attention mechanism. Default is 1 / sqrt(headdim).",
    "causal": "[flash_attn.modules.mha.MHA.forward, tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] Boolean indicating whether the model should use causal masking for self-attention (e.g., during autoregressive generation tasks)",
    "window_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tuple defining left and right context window size in local attention mechanism. Default is (-1,-1) indicating no limit.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean determining whether rotary embeddings are interleaved across dimensions (True for GPT-J style, False otherwise).",
    "alibi_slopes": "[tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] Slope values used in ALiBi bias mechanism to encourage long-range dependencies. Typically passed as a tensor of shape [nheads]."
}

