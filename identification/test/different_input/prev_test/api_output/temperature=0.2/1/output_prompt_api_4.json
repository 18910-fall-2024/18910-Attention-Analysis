{
    "seqlen": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin, benchmarks.benchmark_alibi.flash_rotary.flash_attn_func] The sequence length of the input tensor. It determines the size and shape of tensors q, k, v.",
    "rotary_dim": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin] Dimensionality for rotary positional embeddings applied to query (q) and key (k). This parameter influences the dimension sizes involved in applying rotary position embedding functions like `apply_rotary_emb`.",
    "batch_size": "[flash_attn.flash_attn_interface.FlashAttnFunc.forward, benchmarks.benchmark_alibi.time_fwd_bwd] The batch size of input tensors q, k, v. It determines the first dimension (or shape) of these tensors.",
    "nheads": "[benchmarks.benchmark_alibi.time_fwd_bnd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Number of attention heads in multi-head self-attention mechanism, which affects dimensions and shapes of q, k, v input to `FlashAttention`.",
    "headdim": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Dimensionality per head in multi-head self-attention mechanism. It affects the last dimension of tensors q, k, v and determines their overall shape.",
    "causal": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Whether to apply causal attention mask in self-attention mechanism. This parameter influences the structure of attention model by enabling or disabling future-to-past masking.",
    "dropout_p": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Dropout probability used during training to randomly drop some values in q, k, v tensors. It affects the randomness and stability of attention mechanism.",
    "softmax_scale": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Scaling factor applied before computing softmax over QK^T in self-attention, affecting normalization behavior. It influences the scaling of attention scores.",
    "window_size": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Size of sliding window for local attention mechanism (left and right). This parameter affects how far keys can be attended to from each query position.",
    "alibi_slopes": "[benchmarks.benchmark_alibi.time_fwd_bwd.generate_cos_sin.flash_rotary.flash_attn_func.FlashAttnFunc.forward._flash_attn_forward] Slope values for ALiBi (Attention with Linear Biases) mechanism, which adds biases to attention scores based on relative positions. It influences the positional bias applied in self-attention."
}

