{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Maximum sequence length for the input tokens.",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Number of attention heads in a Transformer model.",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Dimensionality (hidden size) per head for the query/key/value vectors.",
    "dropout_p": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dropout probability used during training to randomly drop some attention weights.",
    "softmax_scale": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Scaling factor for the dot product of query and key vectors before applying softmax. If None, defaults to 1 / sqrt(headdim).",
    "causal": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Boolean indicating whether causal attention mask should be applied.",
    "window_size": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Tuple (left_context_window, right_context_window) defining the local context window size for each query. If (-1,-1), no sliding window is used and full sequence length is considered."
}

