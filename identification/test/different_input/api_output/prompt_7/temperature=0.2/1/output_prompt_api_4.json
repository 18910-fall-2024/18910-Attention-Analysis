{
    "seqlen": "[benchmarks.benchmark_alibi.generate_cos_sin, benchmarks.benchmark_alibi.attn_bias_from_alibi_slopes] The sequence length of the input tensor. It determines the size and shape of tensors such as query (q), key (k), value (v) in attention mechanism.",
    "headdim": "[benchmarks.benchmark_alibi.time_fwd_bwd, benchmarks.benchmark_alibri.flash_rotary] Head dimension for multi-head self-attention mechanisms; it affects tensor dimensions input to `flash_attn_2_cuda` and the model's architecture behavior by determining how information is distributed across different heads.",
    "batch_size": "[benchmarks.benchmark_alibi.time_fwd_bwd, benchmarks.benchmark_alibri.flash_rotary] The batch size of inputs in attention mechanism; it affects tensor dimensions input to `flash_attn_2_cuda` and the model's architecture behavior by determining how many sequences are processed simultaneously.",
    "nheads": "[benchmarks.benchmark_alibi.flops] Number of heads used for multi-head self-attention mechanisms. It determines the size and shape of tensors such as query (q), key (k), value (v) in attention mechanism, affecting model's architecture behavior by distributing information across different parallel processing units.",
    "causal": "[benchmarks.benchmark_alibi.flash_rotary, flash_attn.flash_attn_interface.FlashAttnFunc.forward] A boolean indicating whether to apply causal masking. It affects the computation of attention scores during training or inference with Transformers by restricting each position in a sequence only attending to previous positions.",
    "dropout_p": "[flash_attn.flash_attn_interface.FlashAttnFunc.apply, flash_attn.flash_attn_interface._flash_attn_forward] Dropout probability used for regularization. It affects the model's behavior and prediction quality during training or inference with Transformers by randomly setting a fraction of input tensor elements to zero.",
    "softmax_scale": "[flash_attn.flash_attn_interface.FlashAttnFunc.apply, flash_attn.flash_attn_interface._flash_attn_forward] Scaling factor for QK^T before applying softmax. It affects the computation and normalization of attention scores during training or inference with Transformers by scaling dot products to prevent overflow.",
    "window_size": "[flash_attn.flash_attn_interface.FlashAttnFunc.apply, flash_attn.flash_attn_interface._flash_attn_forward] A tuple indicating left and right window size for local sliding-window self-attention. It affects the computation of attention scores during training or inference with Transformers by limiting each position in a sequence to attend only within its specified context.",
    "alibi_slopes": "[benchmarks.benchmark_alibri.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnFunc.apply] A tensor representing slopes for ALiBi bias. It affects the computation of attention scores during training or inference with Transformers by adding a positional bias to each position's self-attention score.",
    "deterministic": "[flash_attn.flash_attn_interface.FlashAttnFunc.apply, flash_attn.flash_attn_interface._flash_attn_forward] A boolean indicating whether to use the deterministic implementation of backward pass. It affects memory usage and computational speed during training or inference with Transformers by choosing between a faster non-deterministic version and slower but more accurate one."
}

