{
    "embed_dim": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] The dimension of the embedding space (hidden size) in Transformer layers.",
    "num_heads": "[flash_attn.modules.mha.ParallelMHA.__init__, tests.modules.test_mha_parallel.test_mha_parallel] Number of attention heads used for multi-head self-attention mechanism. This parameter affects how input data is split and processed during the computation of attention scores.",
    "head_dim": "[tests.modules.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] The dimensionality (size) of each head in a multi-headed attention layer; it's calculated as embed_dim divided by num_heads. This parameter determines how input data is split across different heads.",
    "rotary_emb_dim": "[flash_attn.modules.mha.ParallelMHA.__init__, tests.modules.test_mha_parallel] Dimensionality for rotary position embeddings, used to apply sinusoidal positional encodings in a rotated manner over the hidden dimension of queries and keys. This parameter influences how attention scores are computed by incorporating relative positions.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Scaling factor applied to QK^T before applying softmax in the computation of attention weights. This parameter affects how normalized scores are computed and can influence model behavior.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Boolean indicating whether to apply a causal mask (masking out future tokens) during attention computation. This parameter determines the type of self-attention mechanism used.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Tuple specifying left and right window size for local (sliding window) attention; -1 means infinite context. This parameter influences the extent of contextual information considered during self-attention.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Boolean indicating whether rotary embeddings are interleaved (combine dimensions 0 & 1, 2 & 3) or not. This parameter affects how sinusoidal positional encodings are applied to queries and keys.",
    "alibi_slopes": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Bias slopes for ALiBi attention mechanism; used in multi-head self-attention computation. These biases influence the scoring of different tokens based on their relative positions."
}

