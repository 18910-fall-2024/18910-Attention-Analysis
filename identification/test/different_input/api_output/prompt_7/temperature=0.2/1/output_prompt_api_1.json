{
    "embed_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension of embeddings, which determines the size and shape of tensors input to flash attention.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel/functools.partial] Number of heads in multi-head self-attention mechanism. Affects model architecture and behavior by determining how many parallel attention mechanisms are used, impacting the dimensions of tensors input to flash attention.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension for rotary embeddings applied during attention computation; affects tensor shapes and sizes related to positional information in self-attention mechanism. It influences how position-awareness is incorporated into each head's query, key, value vectors through rotational transformations.",
    "rotary_emb_interleaved": "[flash_attn.modules.mha.MHA.__init__] A boolean indicating whether rotary embeddings are interleaved (True) or not (False). This parameter determines the structure of how positional information is applied to queries and keys in flash attention computation, affecting tensor shapes during embedding application.",
    "use_flash_attn": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel/functools.partial] A boolean indicating whether Flash Attention mechanism should be used. This parameter affects the behavior of model by enabling or disabling efficient attention computation, impacting how tensors are processed and their dimensions in flash attention.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Indicates if causal masking is applied during self-attention; this boolean hyperparameter influences tensor shapes and sizes related to the mask matrix used for preventing information leakage from future tokens, affecting how tensors are processed within Flash Attention mechanism.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied before computing softmax in attention scores; this hyperparameter affects the computation of attention weights and can influence model behavior by scaling down or up the dot products between query, key vectors.",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tuple indicating left and right window sizes for local (sliding) attention; this hyperparameter affects tensor shapes related to positional information in self-attention mechanism, influencing how far back or forward tokens can attend within a sequence.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean indicating whether rotary embeddings are interleaved; this parameter influences tensor shapes and sizes related to positional information in self-attention mechanism, affecting how position-awareness is incorporated into each head's query, key vectors through rotational transformations."
}

