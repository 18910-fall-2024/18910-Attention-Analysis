{
    "dropout_p": "[flash_attn.flash_attn_interface.FlashAttnFunc.forward, flash_attn.flash_attn_interface.flash_attn_func] Dropout probability for the attention mechanism. It affects regularization and model stability during training.",
    "softmax_scale": "[flash_attn.flash_attn_interface.FlashAttnFunc.forward, flash_attn.flash_attn_interface.flash_attn_func] Scaling factor applied to QK^T before applying softmax in the attention computation process. Affects normalization of scores for better numerical stability or performance optimization.",
    "causal": "[flash_attn.flash_attn_interface.FlashAttnFunc.forward, flash_attn.flash_attn_interface.flash_attn_func] Boolean indicating whether causal masking is applied to prevent future context influence on past tokens during attention computation in sequence modeling tasks like language understanding and generation.",
    "window_size": "[flash_attn.flash_attn_interface.FlashAttnFunc.forward, flash_attn.flash_attn_interface.flash_attn_func] Tuple defining the left and right window size for local sliding-window attention. It restricts each query to attend only within a specified range of keys in sequence modeling tasks.",
    "alibi_slopes": "[flash_attn.flash_attn_interface.FlashAttnFunc.forward, flash_attn.flash_attn_interface.flash_attn_func] Tensor containing slopes used with ALiBi (Attention Layer-wise Information Bottleneck) to bias attention scores for better long-range dependency handling in sequence modeling tasks.",
    "batch_size": "[benchmarks.benchmark_alibi.time_fwd_bwd] The number of sequences processed simultaneously. It determines the first dimension size of input tensors q, k, v passed into `flash_attn_2_cuda` function.",
    "seqlen_q": "[benchmarks.benchmark_alibi.attn_bias_from_alibi_slopes] Length of query sequence in attention mechanism. Determines second dimension sizes for tensor q and output out when calling `flash_attn_2_cuda`. Also used to compute relative position biases with key sequences if causal=False.",
    "seqlen_k": "[benchmarks.benchmark_alibi.attn_bias_from_alibi_slopes] Length of key sequence in attention mechanism. Determines second dimension sizes for tensor k and output out when calling `flash_attn_2_cuda`. Also used to compute relative position biases with query sequences if causal=False.",
    "nheads": "[benchmarks.benchmark_alibi.time_fwd_bwd, benchmarks.benchmark_alibi.flash_rotary] Number of attention heads in the multi-head self-attention mechanism. It determines third dimension sizes for tensors q and k (and v), as well as output out when calling `flash_attn_2_cuda`.",
    "headdim": "[benchmarks.benchmark_alibi.time_fwd_bwd, benchmarks.benchmark_alibi.flash_rotary] Dimension of each attention head in the multi-head self-attention mechanism. It determines last dimension sizes for tensors q and k (and v), as well as output out when calling `flash_attn_2_cuda`."
}

