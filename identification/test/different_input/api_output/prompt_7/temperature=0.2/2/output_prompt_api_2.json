{
    "embed_dim": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] The dimension of the embedding space. It affects the size and shape of tensors input to `flash_attn_2_cuda`.",
    "num_heads": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] Number of attention heads in multi-head self-attention mechanism. Determines how QKV matrices are split and affects the size and shape of tensors input to `flash_attn_2_cuda`.",
    "head_dim": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] The dimension of each attention head. It determines the sizes of QKV matrices and affects how they are processed in `flash_attn_2_cuda`. This is calculated as embed_dim divided by num_heads.",
    "rotary_emb_dim": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] The dimension of rotary embeddings applied to Q and K. It affects the size and shape of tensors input to `flash_attn_2_cuda` when using rotary positional encodings.",
    "softmax_scale": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The scaling factor for QK^T before applying softmax. It affects the computation of attention scores in `flash_attn_2_cuda` and can influence model behavior.",
    "causal": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Whether to apply causal mask for auto-regressive modeling. It affects the computation of attention scores in `flash_attn_2_cuda` and can influence model behavior.",
    "window_size": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The window size for sliding local attention (left, right). It affects the computation of attention scores in `flash_attn_2_cuda` and can influence model behavior.",
    "rotary_interleaved": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Whether to apply rotary embeddings interleaving dimensions. It affects the computation of attention scores in `flash_attn_2_cuda` and can influence model behavior.",
    "alibi_slopes": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Slope values for ALiBi bias. It affects the computation of attention scores in `flash_attn_2_cuda` and can influence model behavior.",
    "num_splits": "[tests.modules.test_mha_parallel.test_mha_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of splits along sequence dimension for key/value matrices. It affects the computation efficiency and memory usage in `flash_attn_2_cuda`."
}

