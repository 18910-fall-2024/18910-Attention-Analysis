{
    "embed_dim": "[flash_attn.modules.mha.MHA.__init__] The embedding dimension of the model, which is also the hidden size.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of attention heads in multi-head self-attention mechanism. Affects how input embeddings are split for parallel computation and interaction with other tensors like Q, K, V.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Dimension of the rotary embedding applied to query (Q) and key (K). This is used for positional encoding in transformers without absolute position embeddings. It affects how Q and K are rotated before computing attention scores.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied to the dot product of query (Q) and key (K). This parameter can affect stability during softmax computation in self-attention mechanisms. Default is 1 / sqrt(headdim), where headdim refers to head dimension.",
    "causal": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean flag indicating whether the attention mechanism should be causal (i.e., each position can only attend to previous positions). This affects how masks are applied during training and inference.",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tuple indicating left and right window size for local (sliding) attention. If set, only keys within the specified range from each query are considered during computation of attention scores.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean flag indicating whether rotary embeddings should be interleaved. This affects how positional encodings are applied to Q and K tensors, influencing the attention mechanism's ability to capture long-range dependencies.",
    "alibi_slopes": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Slope values for ALiBi (Attention with Linear Biases) used in self-attention mechanisms. These slopes are applied to the attention scores, affecting how different positions interact.",
    "cache_seqlens": "[flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence lengths of cached keys and values for each sequence in a batch during incremental decoding. This parameter is crucial when updating the cache with new tokens while maintaining previous context.",
    "cache_batch_idx": "[flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Indices used to index into cached keys and values for each sequence in a batch during incremental decoding. This parameter helps manage the cache efficiently when dealing with sequences of varying lengths.",
    "block_table": "[flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Table used to map block indices to page blocks for paged KV caching mechanism. It is essential in managing memory-efficient attention mechanisms where keys and values are stored in a segmented manner.",
    "num_splits": "[flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of splits along the sequence dimension for key-value tensors during computation. This parameter can affect performance by balancing memory usage with parallelism."
}

