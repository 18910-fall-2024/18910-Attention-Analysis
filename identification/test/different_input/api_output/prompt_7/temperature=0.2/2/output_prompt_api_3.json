{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Sequence length for each sequence in the batch. It determines the size and shape of input tensors like qkv (total number of tokens).",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Number of attention heads.",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dimensionality (or size) of each head in the multi-head self-attention mechanism. It affects tensor shapes and dimensions like qkv.",
    "dropout_p": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dropout probability used during training to prevent overfitting by randomly setting some elements of the input tensors (qkv) to zero. It does not affect tensor shapes but influences attention score computation.",
    "softmax_scale": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Scaling factor for QK^T before applying softmax during the calculation of attention scores. It does not affect tensor shapes but influences how attention weights are computed.",
    "causal": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Boolean indicating whether to apply a causal mask (for auto-regressive modeling). It does not affect tensor shapes but influences the attention mechanism by restricting which tokens can attend to others.",
    "window_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Tuple indicating left and right window sizes for local (sliding) attention. It does not affect tensor shapes but influences the computation of attention scores by limiting which tokens can attend to others.",
    "alibi_slopes": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Bias values used in ALiBi (Attention with Linear Biases) mechanism. It does not affect tensor shapes but influences the computation of attention scores by adding biases to QK^T.",
    "max_seqlen_q": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Maximum sequence length for queries, used in determining padding and memory allocation within `varlen_fwd` function. It affects tensor shapes indirectly through the input tensors' dimensions.",
    "max_seqlen_k": "[flash_attn.flash_attn_interface._flash_attn_varlen_forward] Maximum sequence length for keys, similar to max_seqlen_q but used specifically with key sequences in determining padding and memory allocation within `varlen_fwd` function. It affects tensor shapes indirectly through the input tensors' dimensions."
}

