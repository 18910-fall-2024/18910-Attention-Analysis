{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Maximum sequence length for the input data within one forward pass. It is used to determine padding and attention mask sizes.",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Number of heads in multi-head self-attention mechanism.",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dimensionality (hidden size) per head for the attention layer. It affects tensor dimensions and shapes in `flash_attn_2_cuda`.",
    "dropout_p": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dropout probability used during training to randomly drop some attention weights. It does not affect the model's architecture but influences behavior and prediction quality by introducing randomness.",
    "softmax_scale": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Scaling factor for QK^T before applying softmax. It affects attention score computation during training or inference with the attention mechanism in Transformers.",
    "causal": "[flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.apply, flash_attn.flash_attn_interface._flash_attn_varlen_forward] Boolean indicating whether to apply causal masking. It affects tensor dimensions and shapes by determining padding patterns for attention masks in `flash_attn_2_cuda`.",
    "window_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Tuple indicating the left and right window sizes for local sliding-window self-attention. It affects tensor dimensions by determining padding patterns in `flash_attn_2_cuda`.",
    "alibi_slopes": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Slope values for ALiBi bias. It affects attention score computation by introducing a positional bias to the scores in `flash_attn_2_cuda`."
}

