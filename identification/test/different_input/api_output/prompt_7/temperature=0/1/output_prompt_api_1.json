{
    "embed_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension of the embedding, which is also equal to num_heads * head_dim.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of attention heads in multi-head self-attention mechanism. It affects how input data are split and processed for parallel computation, impacting the model's architecture and behavior.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension used by rotary position embeddings to encode positional information during training or inference with attention mechanisms in Transformers.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Dimension of each head's output, which is the embedding size divided by num_heads. It affects how input data are split and processed for parallel computation, impacting attention score computations.",
    "seqlen": "[flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention] Sequence length of queries (q) in multi-head self-attention mechanism during training or inference with Transformers.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of sequences processed simultaneously. It affects the batch processing and parallel computation efficiency for attention mechanisms.",
    "cache_seqlens": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence lengths used to index into cached keys/values, determining how much context is considered during incremental decoding or training with Transformers' self-attention mechanism.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied before computing the softmax of attention scores. It affects normalization and scaling behavior in multi-head self-attention mechanisms, impacting prediction quality.",
    "causal": "[flash_attn.modules.mha.MHA.__init__] Boolean indicating whether to apply a causal mask during training or inference with Transformers' self-attention mechanism, affecting how past tokens influence future token predictions."
}

