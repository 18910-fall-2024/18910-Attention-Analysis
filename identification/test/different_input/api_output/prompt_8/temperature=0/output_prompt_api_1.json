{
    "embed_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension of the embedding, which is also equal to num_heads * head_dim.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of attention heads in multi-head self-attention mechanism. It affects how input data are split and processed during the computation of Q, K, V matrices for each head.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Dimensionality used in rotary position embeddings. It influences how positional information is encoded and applied to the query (Q) matrix during attention computation, affecting model's ability to capture long-range dependencies.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension of each head within multi-head self-attention mechanism. This parameter determines the size of Q, K, V matrices for individual heads and impacts how scores are computed during attention calculation.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied to the dot product between query (Q) and key (K) vectors before computing softmax. It affects the stability of gradient computation in backpropagation, especially when dealing with large values or high-dimensional embeddings.",
    "causal": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean flag indicating whether to apply a causal mask during attention calculation. When set to True, it restricts each position in the sequence only attending to previous positions (useful for autoregressive models).",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tuple defining left and right window size for local attention. When set, it restricts each position in the sequence to attend only within a sliding window around its own index.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Boolean flag indicating whether rotary embeddings are interleaved. When set, it combines dimensions in pairs (0 & 1, 2 & 3) for applying positional encoding; otherwise, applies to first half and second half separately.",
    "alibi_slopes": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Slope values used in ALiBi (Attention with Linear Biases) mechanism. These biases are added to the attention scores based on relative positions, influencing how much each position attends to others."
}

