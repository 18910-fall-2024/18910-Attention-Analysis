{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch. It determines the size and shape of input tensors such as qkv.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Sequence length for each sequence in the batch. Together with batch_size, it determines the size and shape of input tensors such as qkv.",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Number of attention heads used in multi-head self-attention. It affects the dimensionality of QKV matrices and output tensor dimensions.",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_brdwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dimension of each attention head in multi-head self-attention. It affects the dimensionality of QKV matrices and output tensor dimensions.",
    "dropout_p": "[flash_attn.flash_attn_interface.flash_attn_varlen_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dropout probability used during training to randomly drop some attention scores. It affects the stochasticity and generalization of the model.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_varlen_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Scaling factor applied to QK^T before computing softmax. It affects how attention scores are normalized and can influence the model's behavior.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_varlen_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Boolean indicating whether to apply causal masking. It affects which attention scores are considered in self-attention mechanisms for auto-regressive models.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_varlen_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Tuple indicating the size of sliding window local attention. It affects which keys and values are considered for each query in self-attention mechanisms.",
    "alibi_slopes": "[flash_attn.flash_attn_interface.flash_attn_varlen_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Slope(s) used to create ALiBi bias. It affects the attention scores by adding a positional bias that can influence model behavior."
}

