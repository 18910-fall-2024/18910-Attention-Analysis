{
    "embed_dim": "[tests.modules.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] The dimension of the embedding space. It determines the size and shape of tensors input to `flash_attn_2_cuda`, especially for QKV projection.",
    "num_heads": "[tests.modules.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] Number of attention heads in multi-head self-attention mechanism. This parameter affects both the model's architecture and behavior during training or inference with Transformers by determining how many parallel attention mechanisms operate on different subspaces.",
    "head_dim": "[flash_attn.modules.mha.ParallelMHA._apply_rotary_update_kvcache_attention, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The dimension of each head in multi-head self-attention mechanism. It determines the size and shape of tensors input to `flash_attn_2_cuda`.",
    "rotary_emb_dim": "[tests.modules.test_mha_parallel, flash_attn.modules.mha.ParallelMHA.__init__] Dimension for rotary positional embeddings used during attention computation in Transformers. This parameter influences how position information is incorporated into the model's output performance by modifying Q and K vectors before computing their dot products.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Scaling factor applied to attention scores (QK^T) before applying softmax. This hyperparameter affects the model's behavior and prediction quality by scaling down or up the dot products between query vectors Q and key vectors K.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Boolean indicating whether to apply causal attention mask. This parameter influences how future tokens are masked out in the self-attention mechanism during training or inference with Transformers.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Tuple indicating left and right window size for local (sliding) attention. This parameter affects the model's behavior by limiting each token to attend only within a fixed-size sliding window of tokens.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA.__init__] Boolean indicating whether rotary embeddings are interleaved. This parameter influences how position information is incorporated into the model's output performance by modifying Q and K vectors before computing their dot products."
}

