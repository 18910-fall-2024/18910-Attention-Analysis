{
    "batch_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] The number of sequences in a batch. It affects the model's architecture and behavior by determining how many parallel computations can be performed.",
    "seqlen": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Sequence length or context window size of each sequence in a batch. It influences the model's behavior and prediction quality by controlling how far back into history attention can look.",
    "nheads": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Number of heads for multi-head self-attention. It affects the model's architecture and behavior by distributing computation across multiple parallel attention mechanisms.",
    "headdim": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dimension of each head in multi-head self-attention. It influences the model's architecture and behavior by determining how much information can be processed per attention mechanism.",
    "dropout_p": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Dropout probability for dropping out elements during training. It affects the model's prediction quality by introducing randomness to prevent overfitting and improve generalization.",
    "softmax_scale": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Scaling factor for QK^T before applying softmax. It influences the model's behavior by controlling how attention scores are normalized and can affect prediction quality.",
    "causal": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Boolean indicating whether to apply causal mask for auto-regressive modeling. It affects the model's behavior by controlling attention patterns in sequence generation tasks.",
    "window_size": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Tuple indicating left and right window size for sliding local attention. It influences how far back into history each query can look in a sequence, affecting the model's behavior.",
    "alibi_slopes": "[benchmarks.benchmark_causal.time_fwd_bwd, flash_attn.flash_attn_interface.FlashAttnVarlenQKVPackedFunc.forward] Slope values for ALiBi bias. It affects how attention scores are biased based on distance between queries and keys."
}

