{
    "embed_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] The dimension of the model's embedding, which is also referred to as hidden size in some contexts. It affects how information flows through each layer and influences attention score computation.",
    "num_heads": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The number of heads used for multi-head self-attention, which splits the embedding dimension into multiple smaller dimensions to compute parallel attention scores. It influences how information is distributed across different head-specific representations and affects computation efficiency.",
    "head_dim": "[flash_attn.modules.mha.ParallelMHA.__init__] Dimension per each head in multi-headed attention mechanism; it's derived from embed_dim divided by num_heads, affecting the granularity of representation within individual heads during score calculation.",
    "rotary_emb_dim": "[flash_attn.modules.mha.ParallelMHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The dimension used for rotary position embeddings. It influences how positional information is incorporated into attention scores, affecting the model's ability to capture long-range dependencies.",
    "softmax_scale": "[flash_attn.modules.mha.ParallelMHA.__init__] Scaling factor applied before computing softmax over attention logits; it affects normalization of dot products between query and key vectors during score computation.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA._update_kv_cache_attention] Boolean indicating whether to apply causal masking in self-attention mechanism. It influences the attention scores by ensuring that each token only attends to previous tokens.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA._update_kv_cache_attention] Tuple indicating left and right window size for local (sliding) self-attention. It restricts the attention mechanism's receptive field to a sliding window around each token.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA._update_kv_cache_attention] Boolean indicating whether rotary embeddings are interleaved. It affects how positional information is applied across different dimensions of the query and key vectors.",
    "alibi_slopes": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.ParallelMHA._update_kv_cache_attention] Slope values for ALiBi (Attention with Linear Biases) mechanism. It introduces a bias to the attention scores based on relative positions between tokens."
}

