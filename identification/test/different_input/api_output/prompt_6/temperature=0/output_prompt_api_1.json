{
    "embed_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension of the embedding, which is also equal to num_heads * head_dim.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Number of attention heads in multi-head self-attention mechanism. It affects how input data are split and processed for parallel computation during the forward pass, influencing model performance.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__] The dimension used by rotary position embeddings to encode positional information into queries (Q) and keys (K). This parameter influences attention scores through its impact on how positions are encoded in multi-head self-attention mechanisms, affecting model performance.",
    "rotary_emb_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_atkn_with_kvcache] A boolean indicating whether rotary embeddings should be interleaved. This parameter affects how positional information is encoded into queries and keys through the application of sinusoidal functions, impacting attention scores.",
    "use_flash_attn": "[flash_attn.modules.mha.MHA.__init__] Indicates if Flash Attention mechanism (optimized for efficient computation) will be used in multi-head self-attention. This parameter influences how efficiently computations are performed without changing model output quality directly but is crucial for performance tuning related to attention scores.",
    "causal": "[flash_attn.flash_attn_interface.flash_atkn_with_kvcache] A boolean indicating whether the mechanism should enforce a causal mask, meaning that each position can only attend to previous positions. This parameter influences how queries interact with keys and values in self-attention mechanisms, affecting model performance by enforcing temporal order.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_atkn_interface.flash_attn_with_kvcache] Scaling factor applied before computing the softmax of attention scores. This parameter influences how normalized each score is relative to others in a given sequence, affecting model performance by scaling down or up the influence of individual tokens.",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_atkn_interface.flash_attn_with_kvcache] A tuple indicating left and right window sizes for local attention. This parameter influences how far each position can attend in a sequence, affecting model performance by limiting the context size.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_atkn_interface.flash_attn_with_kvcache] A boolean indicating whether rotary embeddings should be interleaved. This parameter influences how positional information is encoded into queries and keys through the application of sinusoidal functions, impacting attention scores."
}

