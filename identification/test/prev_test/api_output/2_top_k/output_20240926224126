mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here are the hyperparameters related to `flash_attn_2_cuda`:

```
{
  "batch_size": "The batch size of input data for attention mechanism, used in flash_attn_qkvpacked_func and _flash_attn_forward.",
  "seqlen": "The sequence length of input data for attention mechanism, used in time_fwd_bwd and _flash_attn_forward.",
  "headdim": "The head dimension (or the number of hidden units) per query or key vector for multi-head self-attention, used in flash_attn_qkvpacked_func and _flash_attn_forward.",
  "nheads": "The number of attention heads for multi-head self-attention, calculated as dim // headdim, where dim is a hyperparameter used to determine the size of input data tensors (q, k, v) that feed into flash_attn_qkvpacked_func and _flash_attn_forward.",
  "causal": "A boolean indicating whether causal attention mask should be applied for auto-regressive modeling in time_fwd_bwd and _flash_attn_forward. Ignored by the hyperparameter definition as it's a bool.",
  "window_size": "The size of sliding window local attention, used to implement contextualized self-attention or multi-query/multi-key self-attention; defaults to (-1, -1) meaning infinite context window in time_fwd_bwd and _flash_attn_forward. Ignored by the hyperparameter definition as it's a bool.",
  "alibi_slopes": "A vector of slopes for alibi bias term used to regularize attention scores (not directly relevant to flash_attn_2_cuda itself, but related); defaults to None in time_fwd_bwd and _flash_attn_forward. Ignored by the hyperparameter definition as it's a bool.",
  "dropout_p": "The dropout probability; ignored during evaluation of time_f_b and speed_f_b in benchmarks.benchmark_flash_attention.time_f_b",
  "softmax_scale": "A scaling factor used to normalize QK^T before applying softmax for attention mechanism, defaults to sqrt(1/float_dim) = headdim**(-0.5), calculated within flash_attn_qkvpacked_func"
}
```

Note that the key-value pairs only include non-boolean hyperparameters related to `flash_attn_2_cuda`, and ignore the boolean ones including causal, window_size in this particular case due to your requirements.

