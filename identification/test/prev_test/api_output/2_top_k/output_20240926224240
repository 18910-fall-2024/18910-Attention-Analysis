mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=60, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here are the hyperparameters related to `flash_attn_2_cuda`:

```
{
  "batch_size": "The batch size of input tensor, used in flash_attn_qkvpacked_func and _flash_attn_forward",
  "seqlen": "Sequence length of input tensor, used in flash_attn_qkvpacked_func and _flash_attn_forward. It determines the output shape of out (or output) from FlashAttnQKVPackedFunc.apply.",
  "nheads": "Number of attention heads in Transformer architecture, used to determine the size and dimensionality of qkv tensor input to flash_attn_2_cuda",
  "headdim": "Dimension of each head in Transformer architecture. This hyperparameter determines the number of columns (or units) in QKV tensors that are multiplied together.",
  "causal": "A boolean indicating whether causal attention mask is applied during computation, affecting output shape and dimensionality of output from flash_attn_2_cuda",
  "window_size": "(left, right). If not (-1, -1), implements sliding window local attention. This determines the shapes of qkv tensors passed to _flash_attn_forward.",
  "softmax_scale": "The scaling factor for QK^T before applying softmax in flash_attn_2_cuda",
  "dropout_p": "Dropout probability used during forward pass in Transformer, affecting computation and output shape from FlashAttnQKVPackedFunc.apply"
}
```

