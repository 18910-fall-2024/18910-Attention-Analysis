mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=400, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here are the hyperparameters related to `flash_attn_2_cuda`:

```
{
  "batch_size": "The batch size of input tensor, used in flash_attn_qkvpacked_func and _flash_attn_forward",
  "seqlen": "Sequence length of input tensor, used in flash_attn_qkvpacked_func and _flash_attn_forward. It determines the output shape of out (or selfAttention) when calling FlashAttnQKVPackedFunc.apply() or flash_attn_cuda.fwd(). The attention operation will only consider sequences up to this seqlen.",
  "nheads": "Number of attention heads, used in _flash_attn_forward. This is the number of times we repeat self-attention on a particular position and then linearly combine them together",
  "headdim": "Dimension per head (self-attention operation), used in _flash_attn_forward",
  "dropout_p": "Probability to drop out, set it as 0 during evaluation. This is the dropout applied after softmaxed selfAttention scores are computed.",
  "softmax_scale": "Scaling factor for logsumexp of each row of QK^T * scaling (e.g., log of softmax normalization factor), used in flash_attn_qkvpacked_func",
  "causal": "Whether to apply causal attention mask, used in _flash_attn_forward. For instance auto-regressive modeling.",
  "window_size": "(left, right) window size for implementing sliding local windowed selfAttention (similarly with SLiM), used in flash_attn_qkvpacked_func and _flash_attn_forward",
  "alibi_slopes": "Bias added to attention scores of query i and key j. This is the alibi slopes"
}
```
Note that `q`, `k` and `v` tensors are not listed because they correspond to input arguments (`kwargs`) passed directly into flash\_attn_2_cuda function, which means their shapes have been already specified by calling code when invoking FlashAttnQKVPackedFunc.apply() or call of the flash_attn_cuda.fwd().

