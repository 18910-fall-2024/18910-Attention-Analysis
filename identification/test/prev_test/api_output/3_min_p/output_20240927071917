mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here are the hyperparameters related to `flash_attn_2_cuda`:

```
{
    "batch_size": "The batch size of input data. Used in flash_attn_qkvpacked_func, _flash_attn_forward",
    "seqlen": "Sequence length. Used in time_fwd_bwd function call chain and _flash_attn_forward",
    "headdim": "Heads dimension or number of heads for attention mechanism. Used in flops(), efficiency() functions",
    "nheads": "Number of heads, set to dim // headdim as a default value. Used in time_fwd_bwd function call chain and _flash_attn_forward",
    "causal": "Whether causal mask should be applied during the forward pass of attention mechanism.",
                   "Used in flash_attn_qkvpacked_func() and _flash_attn_forward() functions",
    "window_size": "(left, right) tuple. If not (-1, -1), implements sliding window local attention for query at position i will only attend to keys between [i - left, i + right] inclusive.",
                   "Used in flash_attn_qkvpacked_func(), _flash_attn_forward() functions",
    "alibi_slopes": "(nheads,) or (batch_size, nheads) tensor. A bias of (-alibi_slope * |i - j|) is added to attention score of query i and key j.",
                   "Used in flash_attn_qkvpacked_func(), _flash_attn_forward() functions",
    "softmax_scale": "The scaling factor for QK^T before applying softmax during the forward pass. Default value depends on headdim.",
                   "Used in FlashAttnQKVPackedFunc.apply(), _flash_attn_backward() functions"
}
```

