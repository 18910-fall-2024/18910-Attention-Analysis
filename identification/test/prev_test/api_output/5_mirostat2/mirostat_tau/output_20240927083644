mirostat=2, mirostat_eta=0.1, mirostat_tau=3.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of **all** hyperparameters related to `flash_attn_2_cuda`:

```json
{
    "batch_size": "The size of the batch dimension in attention mechanism, used for calculating memory usage and performance. It is set as 32 (default value) by default.",
    "seqlen": "The length of sequence dimension in attention mechanism, used for calculating memory usage and performance. It can be adjusted to different lengths like 512, 1024, etc..",
    "headdim": "The dimensionality of the head in each attention head. In flash_attn_2_cuda function, it is set as 64 by default but also accepts other values such as 128.",
    "nheads": "The number of heads used for multi-head self-attention mechanism inside 'flash_attn_qkvpacked_func' or 'flash_attn_interface._flash_attn_forward'.",
    "causal": "A boolean to indicate whether causal masking should be applied during the attention process. It is set as True by default, meaning that only preceding tokens in sequence are attended.",
    "window_size": "If you want a sliding window local attention mechanism instead of standard self-attention, set this tuple (left_context_window, right_context_window) accordingly",
    "alibi_slopes": "Tuples representing the slopes for adding bias to individual attentions; None will disable slope addition.",
    ¡°softmax_scale¡±:¡°Scaling applied prior softmax operation on output in each attention head. It can be adjusted according to reference value set forward inside qkvpacked_func or also controlled by code flow elsewhere",
}
```

