mirostat=2, mirostat_eta=0.1, mirostat_tau=2.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of **all** hyperparameters that are related to `flash_attn_2_cuda`:

```json
{
    "batch_size": "the batch size used in the attention mechanism (used in _flash_attn_forward())",
    "seqlen": "sequence length, determines the input tensor's shape for flash attn_2_cuda",
    "qkv_shape": "(32, 512, 3, nheads, headdim) etc., which are used to initialize q,k,v tensors before feeding into _flash_attn_forward(), and also used in fwd() function of flash_attn_2_cuda.",
    "window_size": "determines the local attention window size for each query (used in flash_attn_qkvpacked_func())",
    "alibi_slopes": "(nheads,) or (batch_size, nheads) slopes array to add bias to attention scores while using alibi implementation of attetion mechanism. It's used in _flash_attn_forward() and also provided as input argument when calling fwd() function from flash_attn_2_cuda",
    "softmax_scale": "(default 1 / sqrt(headdim)) softmax scaling factor for the QK^T, which is calculated within flash_attn_qkvpacked_func(), then used in _flash_attn_forward(), and finally passed as input argument when calling fwd() function from flash_attn_2_cuda",
    "nheads": "number of attention heads (in _flash_attn_forward())"
}
```

