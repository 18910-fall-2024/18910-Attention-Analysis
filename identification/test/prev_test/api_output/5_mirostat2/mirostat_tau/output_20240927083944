mirostat=2, mirostat_eta=0.1, mirostat_tau=6.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of **all** relevant hyperparameters related to `flash_attn_2_cuda` that I was able to identify:

```json
{
  "window_size": "This is the window size for sliding window local attention. It should be set as (left, right).",
  "alibi_slopes": "A bias of (-alibi_slope * |i - j|) is added to the attention score of query i and key j.",
  "softmax_scale": "The scaling factor used in flash_attn_qkvpacked_func's forward pass to normalize QK^T before applying softmax",
  "dropout_p": "Dropout probability. Should be set to zero during evaluation.",
  "qkv_shape": "(batch_size, seqlen, 3, nheads, headdim). The shape of the input qkv tensor for flash_attn_qkvpacked_func"
}
```

Here are brief descriptions for each parameter:

- `window_size`: This is used in `_flash_attn_forward` to apply sliding window local attention. The context window size can be set as `(left, right)`.

    - **Function**: `_flash_attn_forward`
    - **Role or Purpose**: Apply sliding window local attention

- ``alibi_slopes``: Used in `flash_attn_qkvpacked_func` to add a bias of (-alibi_slope * |i - j|) to the attention score.

    - **Function**: `_flash_fwd_flash_latest`
    - **Role or Purpose**: Add biases to the attention scores, support alibi indices input

- ``softmax_scale``: Used in `FlashAttnQKVPackedFunc.apply` (`_flash_attn_forward`) as scale in softmax for QKV.

    - **Function**: `_flash_fwd_flash_latest`
    - **Role or Purpose**: Scaling factor used to normalize the attention probabilities of each head.

- `dropout_p`: When dropout probability is higher than zero, a mask will be generated by randomly dropping certain attention weights. This parameter decides which part of the model uses dropout and whether it should be enabled during evaluation.

    - **Function**: `flash_attn_qkvpacked_func`
    - **Role or Purpose**: Control the usage of random dropout for each head in flash_attn

- ``qkv_shape``: Specifies how Q, K, V vectors are laid out before applying attention mechanism. It makes sure that all valid inputs into this tensor follow a specific format.

    - **Function**: `flash_attn_qkvpacked_func`
    - **Role or Purpose**: Determines the input layout of qkv tensors
 <!-- {"hyperparameter1": "description", }, is in JSON format, so I added two } at end. -->
}

