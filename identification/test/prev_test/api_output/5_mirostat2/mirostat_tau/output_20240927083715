mirostat=2, mirostat_eta=0.1, mirostat_tau=4.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of **all** hyperparameters related to `flash_attn_2_cuda` along with their descriptions:


```python
{
    "qkv": "Input tensor shape, (batch_size, seqlen, 3, nheads, headdim)",
    "dropout_p": "Dropout probability for attention mechanism in training",
    "softmax_scale": "Scaling factor of QK^T before softmax normalization",
    "causal": "Boolean flag to apply causal mask on the attention weights (1D position embedding effect)",
    "window_size": "Left and right contexts sizes for local sliding window, (-1,-1) means infinite context. Used in `flash_attn_qkvpacked_func`.",
    "alibi_slopes": "(batch_size,nheads), fp32. A bias of -slopes * |i-j| is added to the attention score",
    "deterministic": "Boolean flag for deterministic implementation of bwd pass (slower and more memory-intensive)",
}

```

