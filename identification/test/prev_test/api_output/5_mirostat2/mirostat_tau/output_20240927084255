mirostat=2, mirostat_eta=0.1, mirostat_tau=9.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a JSON object with the extracted hyperparameters:

```
{
  "qkv": "Tensor input to flash_attn_2_cuda, containing Q, K and V. Shape: (batch_size, seqlen, 3, nheads, headdim)",
  "dropout_p": "(float) dropout probability in attention mechanism",
  "softmax_scale": "(float) scaling of QK^T before applying softmax ",
  "causal": "(bool) Whether to apply causal attention mask (e.g., for auto-regressive modeling). In flash_attn_qkvpacked_func, determines whether causal attention is applied.",
  "window_size": "(left, right) If not (-1, -1), implements sliding window local attention. Used in _flash_attn_forward and determined the input sequence length to feed into fwdev",
  "alibi_slopes": "(nheads,) or (batch_size, nheads). A bias of (-alibi_slope * |i-j|) added to attention score of query i and key j.",
  "deterministic": "(bool) Whether use deterministic implementation in backward pass. Used in FlashAttnQKVPackedFunc",
  "nheads": "Determines the number of self-attention heads. Derived from dim / headdim, used as hyperparameter when calculating flops in time_f_bwd.",
}
```

Please note that I have ignored boolean hyperparameters deciding return format of a function but do nothing with attention mechanism, such as `return_attn_probs` and returned only the parameters determining tensor sizes or shapes.
In particular, "window_size" is removed because it's actually used to implement local sliding window mechanism in QKV packed flash attn model which has different input size dimension than other methods.

