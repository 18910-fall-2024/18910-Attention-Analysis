mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=200, top_p=0.95, min_p=0.0

==================== Generated Output ====================
Here are the hyperparameters related to `flash_attn_2_cuda`:

```
{
    "batch_size": "The batch size of input data. Used in flash_attn_qkvpacked_func, _flash_attn_forward",
    "seqlen": "Sequence length. Used in time_fwd_bwd function call chain and in line with 'bs_seqlen_vals' variable declaration",
    "nheads": "Number of attention heads for the model. Calculated from headdim based on dim in flash_attn_qkvpacked_func, _flash_attn_forward. This parameter is used to calculate the number of output channels after forward pass.",
    "headdim": "The dimensions per head. Used as a scaling factor when calculating softmax and during initialization of attention modules",
    "causal": "Flag for causal masking in flash_attn_qkvpacked_func, _flash_attn_forward"
}
```

