mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.1, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{
    "batch_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply, _flash_attn_forward/flash_attn_2_cuda.fwd] The batch size determines the number of sequences processed in parallel. It is a critical dimension for input tensors q, k, v.",
    "seqlen": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply, _flash_attn_forward/flash_attn_2_cuda.fwd] The sequence length determines the number of tokens in each input sequence. It is a critical dimension for input tensors q, k, v.",
    "nheads": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] The number of attention heads in the transformer model. It determines how many parallel self-attention mechanisms are used.",
    "headdim": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] The dimension of each attention head in the transformer model. It affects the size and shape of input tensors q, k, v.",
    "dropout_p": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] The dropout probability used during training to prevent overfitting. It is a hyperparameter that influences the attention mechanism's behavior.",
    "softmax_scale": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] The scaling factor applied to QK^T before applying softmax. It is used for numerical stability and can be set based on the head dimension.",
    "causal": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] Indicates whether to apply a causal mask in self-attention. It is used for autoregressive models where future tokens cannot influence past ones.",
    "window_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] The size of the sliding window in local attention. It determines how far back and forward each query can attend.",
    "alibi_slopes": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func, flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply] The slopes used in the ALiBi (Attention with Linear Biases) mechanism to add biases based on distance. It affects how attention scores are adjusted."
}

