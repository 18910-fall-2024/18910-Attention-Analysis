mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{
    "batch_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/flash_attn_2_cuda.fwd] The number of sequences in a batch.",
    "seqlen": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/flash_attn_2_cuda.fwd] The length or sequence length for each input sequence.",
    "headdim": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Dimension of the attention heads. It determines the size and shape of Q, K, V tensors.",
    "nheads": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Number of attention heads. It influences the shape and size of Q, K, V tensors.",
    "dropout_p": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Dropout probability used during training to prevent overfitting. It does not affect the tensor sizes but is crucial for model regularization.",
    "softmax_scale": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Scaling factor applied to Q and K before computing the attention scores. It does not affect tensor sizes but is important for numerical stability.",
    "causal": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Boolean indicating whether to apply a causal mask. It does not affect tensor sizes but is crucial for autoregressive models.",
    "window_size": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Tuple indicating the left and right window size for local attention. It does not affect tensor sizes but is important for models using sliding windows.",
    "alibi_slopes": "[time_fwd_brd/config/time_fwd_bwd/flash_attn_qkvpacked_func/FastAttnQKVPackedFunc.apply/_flash_attn_forward/flash_attn_2_cuda.fwd] Slope values used in ALiBi bias. It does not affect tensor sizes but is important for models using this type of positional encoding."
}

