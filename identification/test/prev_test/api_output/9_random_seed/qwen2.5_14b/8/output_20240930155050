mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{
    "batch_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/fwd] The number of sequences in a batch. Determines the first dimension size for input tensors q, k, v.",
    "seqlen": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/fwd] Sequence length (the second dimension) of each sequence within a batch in transformers. Determines the second dimensions size for input tensors q, k, v.",
    "nheads": "[flops/time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/fwd] Number of attention heads used by the transformer model layer. This parameter affects the third dimension (number of heads) for input tensors q, k, v.",
    "headdim": "[flops/time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/fwd] Dimension size per attention head used by transformers. This parameter affects the last dimension (dimension of each head) for input tensors q, k, v.",
    "dropout_p": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Dropout probability applied during training to randomly drop attention weights. This parameter is used in the forward pass and does not affect tensor dimensions.",
    "softmax_scale": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Scaling factor for QK^T before applying softmax. This parameter is used in the forward pass and does not affect tensor dimensions.",
    "causal": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Boolean indicating whether to apply causal attention mask. This parameter is used in the forward pass and does not affect tensor dimensions.",
    "window_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Tuple indicating left (first element) and right (second element) window sizes for sliding local attention. This parameter is used in the forward pass to restrict the range of keys that each query attends.",
    "alibi_slopes": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Slope values used for ALiBi (Attention with Linear Biases) mechanism. This parameter is an optional tensor or list that adds bias to attention scores based on the distance between positions.",
    "deterministic": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Boolean indicating whether to use a deterministic implementation for backward pass. This parameter is used in the forward and does not affect tensor dimensions.",
    "return_softmax": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.apply/time_fwd_bwd/flash_attn.flash_attn_interface._flash_attn_forward/fwd] Boolean indicating whether to return softmax results. This parameter is used in the forward pass and does not affect tensor dimensions."
}

