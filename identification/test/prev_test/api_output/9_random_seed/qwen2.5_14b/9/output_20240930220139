mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.5, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{
    "batch_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/flash_attn_2_cuda.fwd] The batch size of the input tensor, determining how many sequences are processed in parallel.",
    "seqlen": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/flash_attn_2_cuda.fwd] Sequence length or number of tokens per sequence. It determines the dimensionality along which attention is computed for each batch.",
    "nheads": "[time_fwd_bwd/flash_attn.flash_attn_interface.FlashAtKVPackedFunc.forward/_flash_attn_forward/flash_attn_2_cuda.fwd] Number of heads in multi-head self-attention mechanism. It splits the input into multiple parallel attention processes, each with a reduced dimensionality.",
    "headdim": "[time_fwd_brdw/time_f/batch_size/seqlen/headdim/nheads/dropout_p/causal/flops/efficiency] The hidden or embedding size per head in multi-head self-attention mechanism. It defines the feature space for each attention process, impacting model capacity and computational cost.",
    "dropout_p": "[time_fwd_bwd/time_f/batch_size/seqlen/headdim/nheads/dropout_p/causal/flops/efficiency] Dropout probability used during training to randomly drop some values from input tensors. It helps prevent overfitting by reducing the co-adaptation of neurons.",
    "softmax_scale": "[time_fwd_bwd/time_f/batch_size/seqlen/headdim/nheads/dropout_p/causal/flops/efficiency] Scaling factor applied before computing softmax in attention mechanism, often set to 1/sqrt(headdim). It normalizes the dot products between queries and keys.",
    "causal": "[time_fwd_bwd/time_f/batch_size/seqlen/headdim/nheads/dropout_p/causal/flops/efficiency] Indicates whether a causal attention mask should be applied, allowing each position to attend only to previous positions. It is used in autoregressive models like language modeling.",
    "window_size": "[time_fwd_bwd/time_f/batch_size/seqlen/headdim/nheads/dropout_p/causal/window_size/flops/efficiency] A tuple defining the left and right context window size for local attention, allowing each position to attend only within a specific range of positions. It is used in models requiring limited context.",
    "alibi_slopes": "[time_fwd_bwd/time_f/batch_size/seqlen/headdim/nheads/dropout_p/causal/window_size/alibi_slopes/flops/efficiency] A tensor containing slopes for ALiBi (Attention with Linear Biases) to add a bias based on distance between positions. It helps in modeling long-range dependencies without positional encodings."
}

