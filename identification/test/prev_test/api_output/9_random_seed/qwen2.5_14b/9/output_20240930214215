mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.5, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{
    "batch_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] The batch size determines the number of sequences processed in parallel, affecting memory and computational requirements.",
    "seqlen": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] The sequence length determines the number of tokens in each input sequence, impacting memory and computational requirements.",
    "headdim": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] The head dimension determines the size of each attention head, influencing model capacity and computational requirements.",
    "nheads": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] The number of attention heads determines the parallelism and model capacity, affecting computational requirements.",
    "dropout_p": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] The dropout probability is used during training to prevent overfitting by randomly setting a fraction of input units to zero.",
    "softmax_scale": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] The scaling factor for the dot product of query and key vectors before applying softmax, which helps in stabilizing gradients.",
    "causal": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] Indicates whether to apply a causal mask, which restricts each position in the sequence from attending to future positions.",
    "window_size": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] Defines the local window size for sliding-window attention, where each query attends to keys within a fixed distance.",
    "alibi_slopes": "[time_fwd_bwd/flash_attn.flash_attn_interface.flash_attn_qkvpacked_func -> flash_attn.flash_attn_interface.FlashAttnQKVPackedFunc.apply -> flash_attn.flash_attn_interface._flash_attn_forward] Slope values for ALiBi (Attention with Linear Biases) mechanism, which adds a bias to the attention scores based on distance."
}

