mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.8, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{ 
  "qkv": "Input tensor for flash_attn_qkvpacked_func, containing Q, K and V matrices. It should be a (batch_size, seqlen, 3, nheads, headdim) shaped tensor.",
  "dropout_p": "Dropout probability used in attention_pytorch function to randomly drop out some of the output units during training",
  "softmax_scale": "Scaling factor for QK^T before applying softmax. Default value is set as sqrt(headdim)",
  "causal": "Boolean indicating whether causal attention mask should be applied or not, typically used in auto-regressive modeling.",
  "window_size": "Tuple representing the left and right window sizes to implement sliding window local attention",
  "alibi_slopes": "Bias of (-alibi_slope * |i - j|) added to the attention score of query i and key j. It can be a (nheads,) or (batch_size, nheads) shaped tensor.",
  "deterministic": "Boolean indicating whether deterministic implementation should be used for backward pass in flash_attn_qkvpacked_func",
  "return_softmax": "Boolean indicating whether to return the attention probabilities during training and evaluation. It's typically set as False."
}

