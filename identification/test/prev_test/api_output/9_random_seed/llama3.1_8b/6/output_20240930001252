mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.8, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input tensor, used for calculating flops and efficiency.", 
"seqlen": "The sequence length of input tensor, used for calculating flops and efficiency.",
"Headdim": "The head dimension in the Transformer model. Used to calculate softmax_scale and flops",
"deterministic": "Whether to use a deterministic implementation of the backward pass (slightly slower) or not.", 
"dropout_p": "Dropout probability during training, used for attention mechanism", 
"softmax_scale": "Scaling factor applied before applying softmax in attention mechanism. Default value is 1 / sqrt(headdim)", 
"causal": "Whether to apply causal attention mask (e.g., for auto-regressive modeling).",
"window_size": "(left, right) tuple indicating the size of sliding window local attention.", 
"alibi_slopes": "Bias added to attention score. Used in flash_attn_qkvpacked_func and _flash_attn_forward", 
"deterministic": "Whether to use a deterministic implementation of the backward pass (slightly slower).",
"return_softmax": "Return softmax probabilities or not, used for testing only.", 
"rng_state": "Random number generator state. Used in flash_attn_qkvpacked_func and _flash_attn_forward"}

