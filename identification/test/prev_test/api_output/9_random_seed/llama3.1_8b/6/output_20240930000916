mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.8, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input data.", 
"seqlen": "The sequence length (number of tokens) in the input data.",
"Headdim": "The dimensionality of each attention head. In Transformers, this is typically set to half the embedding dimention",
"deterministic": "A boolean hyperparameter that determines whether to use a deterministic implementation of the backward pass for training.", 
"return_attn_probs": "A boolean hyperparameter that decides if return attention probabilities or not.",
"softmax_scale": "The scaling factor used in softmax function, typically set as 1/sqrt(Headdim) ",
"deterministic": "Whether to use a deterministic implementation of the backward pass for training",
"causal": "Apply causal mask on input data. Typically True when using autoregressive model.",
"window_size": "(left, right). If not (-1, -1), implements sliding window local attention."}

