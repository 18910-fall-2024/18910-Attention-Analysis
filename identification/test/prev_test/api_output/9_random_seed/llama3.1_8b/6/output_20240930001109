mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.8, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input sequences.", 
"seqlen": "The sequence length (number of tokens) in each input tensor.",
"Headdim": "The dimensionality of the attention head, which determines the number of output features for each token.",
"nheads": "The number of parallel attention heads used to process different parts of the input sequence.", 
"deterministic": "A boolean flag indicating whether to use a deterministic implementation of the backward pass in FlashAttnQKVPackedFunc.",
"return_attn_probs": "A boolean flag indicating whether to return attention probabilities when calling flash_attn_qkvpacked_func or _flash_attn_forward.", 
"softmax_scale": "The scaling factor for softmax normalization, which can be set manually as an input argument or computed internally based on the head dimensionality. If None (default), it is calculated using sqrt(head_dim).", 
"deterministic": "A boolean flag indicating whether to use a deterministic implementation of the backward pass in FlashAttnQKVPackedFunc.", 
"window_size": "The size of sliding window local attention, which can be set as (-1,-1) for global self-attention or specific values (left,right) for local attention. If not provided, it defaults to infinite context window."}

