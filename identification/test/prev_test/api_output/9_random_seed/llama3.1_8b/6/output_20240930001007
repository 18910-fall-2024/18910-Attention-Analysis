mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.8, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input sequences, used as a dimension for attention computation.", 
"seqlen": "The length of each sequence in the batch, also serving as an axis for attention calculations.",
"HeadDim": "A hyperparameter determining the head dimensions within multi-head self-attention. It influences how information is distributed across different heads and processed independently before being aggregated.",
"NHeads": "Number of attentional 'heads' used to process input sequences in parallel, which can be thought about as separate channels for feature extraction and transformation.", 
"causal": "A boolean indicating whether the model should apply causal masking during self-attention. This is particularly useful when modeling sequential data where each element only depends on its left context.",
"deterministic": "An optional flag that determines how gradients are computed in the backward pass of attention computation, with deterministic being slightly slower and more memory-intensive.", 
"return_attn_probs": "A boolean controlling whether to return attention probabilities alongside output. This is typically used for testing purposes rather than during actual inference or training."}

