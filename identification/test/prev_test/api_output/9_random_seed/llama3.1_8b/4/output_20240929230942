mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.85, min_p=0.4

==================== Generated Output ====================
{"qkv": "input tensor for attention mechanism, size of (batch_size, seqlen, 3, nheads, headdim)", 
"dropout_p": "probability of dropout during training or inference", 
"softmax_scale": "scaling factor for softmax normalization in attention score computation",
"causal": "whether to apply causal attention mask for auto-regressive modeling",
"window_size": "size of sliding window local attention (left, right), if not (-1, -1) implements sliding window local attention",
"alibi_slopes": "bias slope used in alibi implementation of attention score computation", 
"deterministic": "whether to use deterministic implementation of backward pass for attention mechanism",
"return_attn_probs": "whether to return attention probabilities during testing"}

 

 

