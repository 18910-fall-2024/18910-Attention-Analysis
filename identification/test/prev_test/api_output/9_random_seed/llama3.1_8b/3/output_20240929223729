mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=30, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input tensor qkv, which is used to calculate the output shape and determine the number of attention heads.", 
"seqlen": "The sequence length of input tensor qkv, which determines the spatial dimension of the attention mechanism.",
"HeadDim": "The head dimension of input tensor qkv, which affects the computation cost and memory usage in flash_attn_2_cuda",
"Nheads": "The number of attention heads, used to parallelize the self-attention operation. It is calculated based on Headdim and dim.", 
"causal": "Whether to apply causal mask during self-attention calculation.",
"deterministic": "A boolean flag that determines whether to use a deterministic implementation for backward pass in flash_attn_2_cuda",
"dropout_p": "The dropout probability used to randomly drop out the attention weights. It is set to 0.0 during evaluation.", 
"softmax_scale": "The scaling factor of QK^T before applying softmax, which can be adjusted based on Headdim.",
"window_size": "A tuple representing the size of sliding window for local attention calculation."}

