mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=30, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input tensors.", 
"seqlen": "The sequence length of input tensors.", 
"Headdim": "The head dimension (or attention head dimension) in the Transformer model, which determines the number of parallel attention heads.",
"Nheads": "The total number of attention heads in the Transformer model.",
"Causal": "A boolean indicating whether to apply causal attention mask for auto-regressive modeling. If True, query at position i will only attend to keys between [i - window_size[0], i + window_size[1]] inclusive.", 
"Window_size": "The size of sliding window local attention if not (-1, -1), which determines the context window for each query.",
"Deterministic": "A boolean indicating whether to use deterministic implementation of backward pass in FlashAttnQKVPackedFunc. If True, it will be slightly slower and uses more memory.", 
"Return_softmax": "A boolean indicating whether return attention probabilities (softmax) if dropout_p > 0", 
"Alibi_slopes": "The slopes used to compute alibi biases for each head in the Transformer model."}

