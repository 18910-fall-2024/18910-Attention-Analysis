mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=30, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input data, used for calculating flops and time.", 
"seqlen": "The sequence length of input data, used for calculating flops and time.",
"HeadDim": "The dimensionality of each head in the attention mechanism, used to calculate softmax_scale and alibi_slopes",
"nheads": "The number of heads in the multi-head self-attention layer, calculated from Headdim and dim",
"causal": "Whether causal attention mask should be applied.",
"deterministic": "Whether to use deterministic implementation for backward pass.", 
"return_attn_probs": "Whether to return attention probabilities."}

