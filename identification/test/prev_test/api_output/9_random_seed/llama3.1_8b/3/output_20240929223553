mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=30, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{ 
    "qkv_shape": "The shape of input tensor q, k and v for flash_attn_qkvpacked_func. It should be (batch_size, seqlen, 3, nheads, headdim)",
    "dropout_p": "Dropout probability in attention_pytorch function.",
    "softmax_scale": "Scaling factor used to normalize the dot product of QK^T before applying softmax",
    "causal": "Whether to apply causal attention mask (e.g., for auto-regressive modeling) ",
    "window_size": "If not (-1, -1), implements sliding window local attention. Query at position i will only attend to keys between [i - window_size[0], i + window_size[1]] inclusive.",
    "alibi_slopes": "A bias of (-alibi_slope * |i - j|) is added to the attention score of query i and key j",
    "deterministic": "Whether to use the deterministic implementation of the backward pass, which is slightly slower and uses more memory. The forward pass is always deterministic.",
    "return_attn_probs": "Whether to return the attention probabilities in flash_attn_qkvpacked_func function for testing only."
}

