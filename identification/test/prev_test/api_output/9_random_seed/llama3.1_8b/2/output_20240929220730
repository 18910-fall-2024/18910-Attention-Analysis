mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=64, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{ 
  "qkv_shape": "Input tensor shape for flash_attn_2_cuda, which should be (batch_size, seqlen, 3, nheads, headdim)",
  "dropout_p": "Dropout probability used in attention mechanism",
  "softmax_scale": "Scaling of QK^T before applying softmax",
  "causal": "Whether to apply causal attention mask for auto-regressive modeling",
  "window_size": "Window size for sliding window local attention, if not (-1, -1)",
  "alibi_slopes": "Bias added to the attention score based on |i-j| in alibi implementation",
  "deterministic": "Whether to use deterministic backward pass or not",
  "return_softmax": "Flag indicating whether return softmax output for testing purposes"
}

