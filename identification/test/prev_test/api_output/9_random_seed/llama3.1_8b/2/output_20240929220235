mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=64, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input sequence, used for calculating attention scores.", 
"seqlen": "The length of the input sequence, used to calculate attention weights and output shape.",
"Headdim": "The dimensionality of each head in multi-head self-attention mechanism, affects output tensor's last dimension",
"nheads": "Number of heads in multi-head self-attention mechanism, determines output tensor's middle dimensions", 
"causal": "Whether to apply causal attention mask for auto-regressive modeling.",
"window_size": "If not (-1,-1), implements sliding window local attention. Query at position i will only attend to keys between [i - window_size[0], i + window_size[1]] inclusive.", 
"deterministic": "Whether to use the deterministic implementation of backward pass in FlashAttnQKVPackedFunc, affects stability and memory usage during training.",
"return_attn_probs": "Whether to return attention probabilities for testing purposes only."}

