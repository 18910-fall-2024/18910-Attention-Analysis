mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=64, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input tensors.", 
"seqlen": "The sequence length of input tensors.",
"HeadDim": "The head dimension used in the Transformer model, which is also known as `headdim` or `dim`.",
"Nheads": "The number of attention heads used in the Transformer model, calculated from Headdim and batch_size.",
"causal": "A boolean hyperparameter indicating whether to apply causal attention mask for auto-regressive modeling.",
"window_size": "A tuple representing the size of sliding window local attention. If set to (-1, -1), it means infinite context window.",
"deterministic": "A boolean hyperparameter used in the backward pass of `flash_attn_qkvpacked_func` indicating whether to use deterministic implementation or not.", 
"return_softmax": "A boolean hyperparameter used in `flash_attn_qkvpacked_func` and `_flash_attn_forward` indicating whether to return softmax output or not.",
"deterministic": "A boolean hyperparameter used in the backward pass of `flash_attn_qkvpacked_func` indicating whether to use deterministic implementation or not.", 
"dropout_p": "The dropout probability used for attention mechanism."}

