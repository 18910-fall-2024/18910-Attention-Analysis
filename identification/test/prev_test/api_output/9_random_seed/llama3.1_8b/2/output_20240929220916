mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=64, repeat_penalty=1.2
temperature=1.0, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{ 
    "qkv_shape": "The shape of input tensor qkv, which is (batch_size, seqlen, 3, nheads, headdim) in the function flash_attn_qkvpacked_func. It determines the sizes and dimensions of tensors input to `flash_attn_2_cuda`.",
    "softmax_scale": "A scaling factor used for softmax normalization in attention mechanism in the function _flash_attn_forward. The value is set as 1 / sqrt(headdim) by default, but can be customized based on specific use cases",
    "causal": "Whether to apply causal attention mask (e.g., for auto-regressive modeling). It's a boolean hyperparameter used in the function _flash_attn_forward and determines whether `flash_attn_2_cuda` should perform self-attention or not.",
    "window_size": "(left, right) tuple indicating sliding window size. If set to (-1,-1), it means no sliding window is applied; otherwise, query at position i will only attend to keys between [i - left, i + right] inclusive in the function _flash_attn_forward",
    "alibi_slopes": "(nheads,) or (batch_size,nheads) tensor of alibi slopes used for attention bias. It's a hyperparameter determining how much each query-key pair contributes to final output and is used in `flash_attn_2_cuda`.",
    "dropout_p": "Dropout probability, which should be set to 0 during evaluation but can be customized based on specific use cases",
    "nheads": "Number of attention heads. It's a hyperparameter that determines the structure of attention model and is used in `flash_attn_2_cuda`.",
    "headdim": "Dimension per head, which should match with input dimension (e.g., embedding dim). This parameter affects how much computation each query-key pair performs",
    "batch_size" :  "The number of samples or sequences processed together. It's a hyperparameter that determines the batch size and is used in `flash_attn_2_cuda`.",
    "seqlen": "Sequence length, which should match with input sequence length (e.g., sentence length). This parameter affects how much computation each query-key pair performs",
    "softmax_scale=None":  "A scaling factor used for softmax normalization. The value is set as None by default and will be calculated automatically based on headdim"
}

