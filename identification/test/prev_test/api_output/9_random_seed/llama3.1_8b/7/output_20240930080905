mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.9, seed=-1, tfs_z=0.94, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input sequence.", 
"seqlen": "The length of input sequence.", 
"HeadDim": "The dimensionality of attention head.",
"Nheads": "Number of parallel attention heads in a transformer model.",
"Causal": "Whether to apply causal mask for auto-regressive modeling or not. ",
"Deterministic": "Whether to use deterministic implementation of backward pass, which is slightly slower and uses more memory.", 
"Return_attn_probs": "Whether to return the attention probabilities during testing only."}

