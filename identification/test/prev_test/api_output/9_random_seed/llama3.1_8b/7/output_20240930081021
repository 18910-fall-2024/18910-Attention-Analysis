mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.9, seed=-1, tfs_z=0.94, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input sequence.", 
"seqlen": "The length of input sequence.",
"Headdim": "The dimensionality of attention head. It is used to calculate the scaling factor for softmax and also determines the output shape of flash_attn_2_cuda.",
"Nheads": "The number of attention heads in multi-head self-attention mechanism.", 
"causal": "Whether to apply causal mask on input sequence, which prevents future information from being accessed by current position. It is used as a hyperparameter for training and inference with Transformer models.",
"deterministic": "A boolean flag indicating whether the backward pass should be deterministic or not in flash_attn_2_cuda.", 
"dropout_p": "The dropout probability of attention mechanism, which randomly sets some output elements to zero during forward pass. It is used as a hyperparameter for training and inference with Transformer models.",
"window_size": "A tuple representing the size of sliding window local attention. If not (-1,-1), it implements sliding window local attention in flash_attn_2_cuda.", 
"softmax_scale": "The scaling factor applied to QK^T before softmax operation in flash_attn_2_cuda."}

