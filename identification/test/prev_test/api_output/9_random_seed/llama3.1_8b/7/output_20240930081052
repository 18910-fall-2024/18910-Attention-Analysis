mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.9, seed=-1, tfs_z=0.94, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"qkv": "Input tensor shape: (batch_size, seqlen, 3, nheads, headdim). The input is a stack of Q, K, V tensors.", 
"dropout_p": "Dropout probability for attention mechanism. Should be set to 0.0 during evaluation.",
"softmax_scale": "Scaling factor for softmax function in attention mechanism (default: sqrt(headdim)).",
"causal": "Whether to apply causal attention mask (e.g., for auto-regressive modeling).",
"window_size": "Sliding window size for local attention, if not (-1, -1), implements sliding window local attention.",
"alibi_slopes": "Bias of alibi slopes added to the attention score. Used in flash_attn_qkvpacked_func and _flash_attn_forward functions.",
"deterministic": "Whether to use deterministic implementation of backward pass (slower but uses more memory).",
"return_softmax": "Whether to return softmax probabilities, used for testing only."}

