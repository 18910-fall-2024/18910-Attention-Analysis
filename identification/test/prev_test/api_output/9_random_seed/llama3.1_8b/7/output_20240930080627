mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.9, seed=-1, tfs_z=0.94, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{ 
  "qkv": "Input tensor shape, a hyperparameter determining the sizes and dimensions of tensors input to flash_attn_2_cuda. It should be (batch_size, seqlen, 3, nheads, headdim).",
  "dropout_p": "Dropout probability for attention mechanism in training phase.",
  "softmax_scale": "Scaling factor used before applying softmax function during forward pass of flash_attn_qkvpacked_func and _flash_attn_forward. It should be a float value.",
  "causal": "Boolean hyperparameter indicating whether to apply causal attention mask, which is useful for auto-regressive modeling in Transformers.",
  "window_size": "Tuple containing two integers (left, right) representing the size of sliding window local attention used during forward pass of flash_attn_qkvpacked_func and _flash_attn_forward. If it's (-1, -1), then no local attention is applied.",
  "alibi_slopes": "List or tensor of slopes for alibi bias in attention mechanism. It should be a hyperparameter determining the structure of attention model used during forward pass of flash_attn_qkvpacked_func and _flash_attn_forward.",
  "deterministic": "Boolean hyperparameter indicating whether to use deterministic implementation of backward pass, which is slightly slower but uses less memory.",
  "return_softmax": "Boolean hyperparameter deciding whether to return softmax probabilities along with output during forward pass of flash_attn_qkvpacked_func and _flash_attn_forward."
}

