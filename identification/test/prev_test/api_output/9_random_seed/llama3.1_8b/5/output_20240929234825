mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.9, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input data.", 
"seqlen": "The sequence length of input data, which is also the number of tokens in a single sample.",
"Headdim": "The dimensionality of each attention head. In other words, it's the embedding dimension for QKV.", 
"deterministic": "A boolean flag to indicate whether to use the deterministic implementation of backward pass or not.", 
"dropout_p": "Dropout probability used during training process.", 
"softmax_scale": "Scaling factor applied before softmax operation in attention mechanism. Default is 1/sqrt(Headdim).", 
"deterministic": "A boolean flag indicating if we want to use deterministic implementation of backward pass or not."}

