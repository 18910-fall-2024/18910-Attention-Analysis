mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.9, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
{"batch_size": "The batch size of input tensor qkv, which determines the number of sequences to be processed simultaneously.", 
"seqlen": "The sequence length (number of tokens) in each sequence. It is used as a dimension for attention calculation and padding.",
"Headdim": "The hidden dimension of QKV vectors, determining the embedding size per head",
"nheads": "Number of heads for multi-head self-attention mechanism",
"causal": "Whether to apply causal mask (e.g., auto-regressive modeling)",
"deterministic": "Implementation mode of backward pass in attention computation. The forward pass is always deterministic.",
"return_attn_probs": "Flag indicating whether to return the attention probabilities, used for testing purposes only",
"window_size": "(left,right) tuple specifying sliding window local attention parameters",
"alibi_slopes": "Bias added to attention score based on distance between query and key positions. It is an array of slopes (one per head)",
"deterministic": "Implementation mode of backward pass in attention computation.",
"dropout_p": "Dropout probability for scaling the output tensor."}

