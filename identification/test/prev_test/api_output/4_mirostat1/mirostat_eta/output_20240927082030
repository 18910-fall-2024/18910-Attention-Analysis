mirostat=1, mirostat_eta=0.9, mirostat_tau=3.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of hyperparameters related to `flash_attn_2_cuda`:

```json
{
    "qkv_shape": "Input tensor shape: (batch_size, seqlen, 3, nheads, headdim) for qkvpacked_func and its variants.",
    "dropout_p": "(Optional) dropout probability. Default to not dropping any outputs in evaluation mode.",
    "causal": "(bool or tuple (-1,-1)) Whether to apply causal attention mask (e.g., for auto-regressive modeling). By default, it's set as False.",
        (Note: window_size is a tuple of two integers indicating the left and right boundaries respectively. When both are -1, that means infinite context windows)",
    "softmax_scale": "(float) The scaling of QK^T before applying softmax. Default to 1 / sqrt(headdim).",
    "nheads": "If headdim is passed in as an argument or config parameter, it should be a divisor for nheads. Otherwise nheads = dim // headdim.",
     (Note: alibi_slopes can only be set when both causal and window_size are not (-1,-1). It's added to the attention score of query i at key j if applicable)
}
```

