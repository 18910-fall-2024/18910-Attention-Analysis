mirostat=1, mirostat_eta=0.2, mirostat_tau=3.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of **hyperparameters** related to `flash_attn_2_cuda`:

```
{
    "batch_size": "The number of sequences in the input tensor, used as an argument for torch.randn() and flash_attn_qkvpacked_func.",
    "causal": "A boolean indicating whether causal attention mask should be applied. Default value is False.',
    'seqlen': "Sequence length for each sequence element when calling `torch.randn()` and `flash_attn_qkvpacked_func`, setting the second dimension of input tensors in flash_attn_2_cuda.",
    "nheads": "The number of multi-head attention heads, derived from headdim. Used to slice qkv tensor into three blocks (Q, K, V) then apply forward/backward pass through `flash_attn_qkvpacked_func` and `flash_attn_2_cuda`.",
    'headdim': "Each head's feature dimension after parallelizing key-value pairs in flash_attn interface. This value not only inherited from input tensor but also influenced the throughput of ``flash_attn_interface._flash_attn_forward``.",
    "soft_scal": "A float specifying how to scale QK^T scores before applying softmax",
}
```

