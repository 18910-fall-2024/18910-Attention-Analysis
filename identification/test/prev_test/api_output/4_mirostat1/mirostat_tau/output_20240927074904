mirostat=1, mirostat_eta=0.1, mirostat_tau=0.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a JSON with the extracted hyperparameters related to `flash_attn_2_cuda`:

```
{
  "qkv": "Input tensor (batch_size, seqlen, 3, nheads, headdim) for attention mechanism in flash_attn_2_cuda.",
  "dropout_p": "Dropout probability used during training of attention mechanism in flash_attn_2_cuda. Default value is set to zero when not using dropout.",
  "softmax_scale": "Scaling factor applied to QK^T before applying softmax operation, which determines the size and shape of input tensor qkv.",
  "causal": "Flag indicating whether causal attention mask should be used in flash_attn_2_cuda. Default value is False unless specified otherwise by user.",
  "window_size": "(left, right) tuple specifying sliding window local attention context, determining sizes and shapes of input tensors to flash_attn_2_cuda",
  "alibi_slopes": "Bias slopes for alibis used in attention mechanism [optional], determines the size and shape of output tensor out.",
  "deterministic": "[flag] whether deterministic implementation should be used when computing gradients with respect to inputs q, kv. If False, a faster but non-deterministic variant is employed",
}
```

