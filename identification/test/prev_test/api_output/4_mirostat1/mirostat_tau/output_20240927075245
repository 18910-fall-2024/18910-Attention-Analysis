mirostat=1, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.2
temperature=0.8, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is a list of **all** relevant hyperparameters related to `flash_attn_2_cuda`:

```
{
    "qkv":  # Input tensor shape
        {
            "batch_size": 
                {"scope": "_flash_attn_forward", "description": "The number of independent samples in the dataset."},
            "seqlen": 
                {"scope": "_flash_attn_forward", "description": "Sequence length."},  
            "nheads":
                {"scope": time_fwd_bwd, "description": "Number of attention heads"},
            "headdim":
                {"scope": flops, "description": "Size of each head."},
        },
    "window_size":  # Window size for local attention
        {
            "left": 
                {"scope": _flash_attn_forward,"description":"For "},
            "right":  
                {"scope":_flash_attn_forward , "description":"",
         },
    "dropout_p":
        {   
             "scope":attention_pytorch,
             "description": "The probability of an element to be zeroed during training."
         } ,
     "softmax_scale" : 
        {
            "scope":"FlashAttnQKVPackedFunc.apply",
                "description":

           },
    "causal":
        {   
              "scope":flash_attn_qkvpacked_func,
             "description": "Whether to apply causality mask."},
}
```

Note: I've formatted the output as JSON.

