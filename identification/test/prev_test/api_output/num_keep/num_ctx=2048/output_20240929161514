mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=2048, num_keep=35, repeat_last_n=64, repeat_penalty=1.2
temperature=1.0, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
Here is a list of **hyperparameters** related to `flash_attn_2_cuda`:

```
{
    "softmax_scale": "A scale factor used in the softmax function, calculated as qkv.shape[-1] ^ (-0.5) if not provided (used in FlashAttnQKVPackedFunc.forward)",
    "dropout_p": "The dropout probability for attention mechanism (used in FlashAttnQKVPackedFunc.forward and _flash_attn_forward)",
    "causal": "A boolean indicating whether to use causal masking in the attention mechanism, or not",
    "window_size": "A tuple of two integers representing the window size used in the attention mechanism (used in FlashAttnQKVPackedFunc.forward and _flash_attn_forward)",
    "alibi_slopes": "The slopes for alibi computation (used in FlashAttnQKVPackedFunc.forward and _flash_attn_forward)",
    "deterministic": "A boolean indicating whether to use deterministic mode or not",
}
```

These **hyperparameters** are used to determine the sizes, dimensions, shapes of input tensors, attention model structure, etc.

