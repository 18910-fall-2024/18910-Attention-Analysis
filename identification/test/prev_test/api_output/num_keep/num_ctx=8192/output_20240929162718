mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=8192, num_keep=25, repeat_last_n=64, repeat_penalty=1.2
temperature=1.0, seed=0, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.9, min_p=0.4

==================== Generated Output ====================
Here are the **hyperparameters** related to `flash_attn_2_cuda`:

```json
{
    "qkv_shape": "(batch_size, seqlen, 3, nheads, headdim)",
    "dropout_p": "float. Dropout probability.",
    "softmax_scale": "1 / sqrt(headdim). The scaling of QK^T before applying softmax.",
    "causal": "bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).",
    "window_size": "(left, right) If not (-1, -1), implements sliding window local attention.",
    "alibi_slopes": "fp32. A bias of (-alibi_slope * |i - j|) is added to the attention score of query i and key j."
}
```

Here's a brief description for each parameter:

* `qkv_shape`: The shape of input tensor qkv, which determines the sizes, dimensions, or shapes of tensors input to flash_attn_2_cuda. (Used in function `_flash_attn_forward`)
* `dropout_p`: Dropout probability used during training with attention mechanism. (Used in functions `attention_pytorch`, `time_fwd_bwd` and `flash_attn_qkvpacked_func`)
* `softmax_scale`: The scaling of QK^T before applying softmax, which determines the output scale of flash_attn_2_cuda's forward pass.
	+ It is set to 1 / sqrt(headdim) by default. (Used in functions `_flash_attn_forward` and `attention_pytorch`)
* `causal`: Whether to apply causal attention mask for auto-regressive modeling, which determines the output of flash_attn_2_cuda's forward pass.
	+ It is set to False or True depending on different scenarios. (Used in functions `time_fwd_bwd`, `_flash_attn_forward` and `attention_pytorch`)
* `window_size`: The size of sliding window local attention implemented by flash_attn_2_cuda, which determines the output scale of flash_attn_2_cuda's forward pass.
	+ It is set to (-1, -1) by default. (Used in functions `_flash_attn_forward` and `attention_pytorch`)
* `alibi_slopes`: The bias added to attention scores for query i and key j during training with attention mechanism.
	+ It is used only when implementing alibi attention. (Used in function `_flash_atnn_forward`)

