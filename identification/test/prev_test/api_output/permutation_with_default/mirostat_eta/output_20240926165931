mirostat=1, mirostat_eta=0.5, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.1
temperature=0.8, seed=0, tfs_z=1.0, num_predict=512
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is the list of hyperparameters that affect `flash_attn_2_cuda`:

```json
{
    "qkv_shape": "(batch_size, seqlen, 3, nheads, head_dim)",
    "softmax_scale": None or (headdim)^(1/2), the scaling of QK^T before applying softmax,
    "causal": bool, whether to apply causal attention mask,
    "window_size": (-1,-1), if not (-1,-1), implements sliding window local attention,
    "nheads": dim // headdim, number of attention heads,
    "dropout_p": 0. or float in [0., 1.), dropout probability for training
}
```

I ignored the boolean hyperparameters that only decide return format of a function but do nothing with the attention mechanism, e.g., `return_attn_probs` and `deterministic`.

