mirostat=1, mirostat_eta=0.1, mirostat_tau=8.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.1
temperature=0.8, seed=0, tfs_z=1.0, num_predict=512
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here is the list of hyperparameters related to `flash_attn_2_cuda`:

```json
{
  "qkv_shape": "Input tensor shape (batch_size, seqlen, 3, nheads, headdim) for flash attention",
  "dropout_p": "Dropout probability used in attention mechanism",
  "softmax_scale": "Scaling of QK^T before applying softmax in attention mechanism",
  "causal": "Whether to apply causal attention mask in auto-regressive modeling",
  "window_size": "Sliding window size for local attention (left, right)",
  "alibi_slopes": "Bias slopes used in alibi-based attention mechanism",
  "deterministic": "Whether to use deterministic implementation of backward pass in flash attention",
  "return_softmax": "Option to return softmax probabilities in flash attention"
}
```

Here are the details:

- `qkv_shape`: This is determined by the input tensors `q`, `k`, and `v` in the `_flash_attn_forward` function. It's a tuple of `(batch_size, seqlen, 3, nheads, headdim)`.

- `dropout_p`: This is used to calculate dropout probabilities for attention mechanism. It can be set to different values for training or inference modes. You can find it in the `flash_attn_qkvpacked_func` function and `_flash_attn_forward`.

- `softmax_scale`: This scaling factor is used when calculating softmax normalization factors. In `flash_attn_qkvpacked_func`, if you don't specify a value, the `softmax_scale` will be set to $\sqrt{\text{headdim}}$.

- `causal`: A boolean indicating whether attention mask should consider causal relationships between tokens in sequence.

- `window_size`: This defines the boundaries for local attentions. When using flash-based attentions, you might need to specify window sizes (left and right boundaries) depending on your data structure.

- `alibi_slopes`: The bias slopes used in alibi-based attention mechanism.
 
- `deterministic`: A boolean indicating whether flash attention's backward pass is deterministic or not. This hyperparameter only matters when doing backpropagation.

- `return_softmax`: An option flagging whether you want to return softmax probabilities after running the flash based attentions.

