mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=4608, repeat_last_n=64, repeat_penalty=1.6
temperature=0.8, seed=0, tfs_z=1.0, num_predict=512
top_k=40, top_p=0.9, min_p=0.0

==================== Generated Output ====================
Here are some of these **HyperParameters** related with `flash_attn_2_cuda`.


```
{

    "qkv": {
        "_Description_: The input tensor to flash at attention model."
        
    },
    
     window_size: {

       "*Role/Purpose:_ Determines the sizes and shapes for applying causal or local attentions when inputs are passed into FlashAtntQKVPackedFunc.apply"
      },

"headdim":{
   "description": "_Description_: The dimension of each head in a multi-head attention. In this task, we set headeDim=64 as an example."
},

    alibi_slopes: {
        "*Role/Purpose:_ Determines the slopes to be added for computing attentions"
       },

"softmax_scale":{
   "description": "_Description_: The scaling factor applied during softmax computation in flash at attention model "
 },


  nheads:{
      "__key__":"the number of heads used when applying multi-head attendtioin. When calling `flash_attn_qkvpacked_func`, we automatically set it as (dim//headdim)"
},

    "dropout_p": {
        "_Description_: The probability to be dropped in flash at attention model during training process."
        
     },
    
      causal:{

         "*Role/Purpose:_ Determines whether or not the attentions should have causality when inputted into FlashAtntQKVPackedFunc.apply",
},
"batch_size":{
    "description":"_Description_: The batch size of inputs fed to flash_attn_qkvpacked_func during forward pass. This is used for calculating attention weights"
 },
 
   seqlen:{
      "__key__": "_The sequence length or time step number when passing into FlashAtntQKVPackedFunc.apply._",
    },

}
```

