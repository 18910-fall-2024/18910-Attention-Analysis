mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.2, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embeddings for each token in a sequence.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used by Multi-Head Attention (MHA).",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head in the multi-head self-attention mechanism.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] Dimension for rotary position embeddings used to modify queries and keys before computing attention scores. This is typically half the head dimension.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.forward, tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] The length of sequences being processed in a batch.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] Number of samples or sequences in the current mini-batch during training/inference.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Scaling factor applied to QK^T before applying softmax. Default is 1 / sqrt(headdim).",
    "causal": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Boolean indicating whether to apply a causal mask in the attention mechanism.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, tests.modules.test_block_parallel.test_block_parallel] Tuple defining left and right window size for local (sliding) attention. -1 means infinite context window."
}

