mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.4, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embeddings or hidden states in a Transformer model.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used by Multi-Head Attention (MHA) mechanism. It determines how many parallel self-attention mechanisms are applied to the input embeddings or hidden states in a Transformer model.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head within multi-head attention, calculated as `embed_dim / num_heads`",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.functools.partial] Dimension used for rotary position embeddings. It affects the positional encoding mechanism in Transformers.",
    "seqlen": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention] Sequence length of input sequences or queries passed to attention mechanisms during training/inference. It determines the size and shape of tensors in self-attention operations.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] Number of samples (or batch) processed at once, which affects tensor sizes when performing parallel computations across multiple GPUs/TPUs.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Scaling factor applied to the dot product of query and key vectors before computing attention scores. It helps stabilize gradients during training by scaling down large values in QK^T.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Boolean flag indicating whether the model uses causal (autoregressive) attention. When set to True, it ensures that each token only attends to previous tokens in a sequence.",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Tuple defining the left and right window size for local (sliding) attention. It restricts the range of keys each query can attend to within a fixed-size sliding window.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Boolean flag indicating whether rotary position embeddings are interleaved across dimensions. When True, it combines even and odd dimension pairs for positional encoding."
}

