mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=1.0, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the embedding or hidden layer in Transformer models.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used in Multi-Head Attention (MHA).",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head within the multi-head self-attention mechanism.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.functools.partial] Dimension used for rotary position embedding in attention layers. It is half of the total dimension if interleaved mode is enabled.",
    "seqlen": "[flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence length or context window size for input sequences in attention mechanisms. It determines the shape of tensors like Q (queries), K (keys), and V (values).",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] The number of samples processed at once during training/inference.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Scaling factor applied to the dot product between query and key vectors before computing attention scores. It is often set as 1/sqrt(head_dim).",
    "window_size": "[tests.modules.test_block_parallel.functools.partial, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Defines a sliding window for local (sliding-window) self-attention mechanisms.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Indicates whether rotary embeddings are interleaved across dimensions. This affects how the embedding is applied to queries and keys."
}

