mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=10, top_p=0.8, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embeddings or hidden states in Transformer models.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used in multi-head self-attention mechanism. This hyperparameter determines how many parallel attention mechanisms are applied to different subspaces (or 'heads') within the input embeddings or hidden states.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head's output, calculated as `embed_dim / num_heads`. It specifies the size of individual heads in multi-head attention mechanisms.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel] The dimensionality for rotary embeddings applied to query and key vectors. This hyperparameter influences how positional information is encoded into the queries and keys through sinusoidal functions.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence length of input sequences in Transformer models or during inference with KV cache. It determines the size along sequence dimension for tensors like query (Q), key (K), and value (V).",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Batch size of input sequences in Transformer models or during inference with KV cache. It determines the batch dimension for tensors like query (Q) key (K), and value (V).",
    "cache_seqlens": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention] Sequence lengths of cached sequences used in KV cache mechanism. It helps manage the sequence length dimension for caching and updating key-value pairs during incremental decoding.",
    "rotary_cos": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention] Cosine part of rotary embeddings applied to query (Q) or key (K). It is used in conjunction with `rotary_sin` for positional encoding.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Scaling factor applied before computing the softmax operation during attention score calculation. This hyperparameter can be set to control the scaling of QK^T (query times key transpose) scores.",
    "causal": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Boolean flag indicating whether causal attention mask is applied. When `True`, it restricts each query to attend only to previous positions in the sequence (useful for autoregressive models).",
    "window_size": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Tuple defining left and right window size limits for local attention. When not (-1,-1), it restricts each query to attend only within a sliding window around its position.",
    "rotary_interleaved": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Boolean flag indicating whether rotary embeddings are interleaved across dimensions. When `True`, it combines even and odd dimension pairs for positional encoding."
}

