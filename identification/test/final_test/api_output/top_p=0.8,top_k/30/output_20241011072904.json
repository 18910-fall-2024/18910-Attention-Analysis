mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=30, top_p=0.8, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embeddings for each token in a sequence.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used by Multi-Head Attention (MHA).",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head in the multi-head self-attention mechanism.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.functools.partial] Dimension used for rotary position embedding. It is half of `head_dim` if not specified otherwise and must be divisible by 16.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] The sequence length for the input tokens in a batch. This is used to determine how much of the cache should be updated and also affects attention computation.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] Number of sequences in a batch. This is used to determine how many rows the cache has for storing past key-value pairs.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] Scaling factor applied before computing attention scores. It is usually set to 1 / sqrt(head_dim) if not specified.",
    "causal": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] Boolean flag indicating whether the attention mechanism should be causal (i.e., each token can only attend to previous tokens).",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] Tuple specifying left and right window size for local attention. If set, the model will only attend to a sliding window of tokens around each token.",
    "cache_seqlens": "[flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] Sequence lengths for cached keys and values. Used when updating the cache with new sequences in incremental decoding scenarios."
}

