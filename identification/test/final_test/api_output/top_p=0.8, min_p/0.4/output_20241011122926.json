mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.8, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the embedding or hidden size in Transformer models.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used for multi-head self-attention mechanism. It determines how many parallel processes are run to compute Q, K, and V matrices.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality (depth) of each head in the MultiHeadAttention layer; it is calculated as `embed_dim / num_heads`.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] The dimension of rotary embeddings used for positional encoding in the attention mechanism. It must be divisible by 16.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.forward, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence length or number of tokens in a sequence for which the attention mechanism is being applied. It affects how many positions each token can attend to.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel] The batch size used during training or inference, indicating the number of sequences processed at once.",
    "softmax_scale": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache, flash_attn.modules.mha.MHA.__init__] Scaling factor applied to QK^T before applying softmax. It is usually set as 1 / sqrt(headdim).",
    "window_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Tuple indicating the left and right window size for local attention mechanism.",
    "cache_seqlens": "[flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence lengths of cached keys/values used in incremental decoding or generation tasks."
}

