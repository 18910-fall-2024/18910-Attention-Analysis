mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.8, min_p=0.4

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embedding or hidden state in Transformer models.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used in multi-head self-attention mechanism. It determines how many parallel attention computations are performed and combined to form a larger output tensor.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality (or depth) of the key, query, value projections for each head within an attention layer in Transformer models.",
    "rotary_emb_dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel._apply_rotary_update_kvcache_attention] The dimensionality of the rotary positional embedding used to modify query and key vectors. It affects how much information about position is embedded into each vector.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] The sequence length of the input tensor in Transformer models; it determines the size along the sequence dimension for attention operations and positional embeddings.",
    "batch_size": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] The number of sequences or samples processed in parallel within a batch; it affects how many independent computations are performed simultaneously during attention operations.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] The scaling factor applied to the dot product of query and key vectors before computing softmax in self-attention mechanism. It helps stabilize gradients during training.",
    "causal": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] A boolean flag indicating whether the attention operation should enforce a causal mask, meaning that each position can only attend to previous positions.",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] A tuple specifying the left and right window size for local attention. It limits each position's ability to attend only within a sliding window of context.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] A boolean flag indicating whether the rotary positional embeddings are interleaved across dimensions. It affects how sine and cosine values are applied to even/odd positions.",
    "alibi_slopes": "[flash_attn.modules.mha.MHA.__init__, flash_atpen.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] A tensor containing slopes for ALiBi (Attention with Linear Biases) bias terms, which are added to the attention scores. It helps in modeling long-range dependencies.",
    "cache_seqlens": "[flash_atpen.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache._apply_rotary_update_kvcache_attention] A tensor indicating sequence lengths for cached keys and values, used to manage incremental decoding in Transformer models."
}

