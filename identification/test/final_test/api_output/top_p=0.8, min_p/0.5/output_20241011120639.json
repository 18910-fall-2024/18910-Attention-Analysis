mirostat=0, mirostat_eta=0.1, mirostat_tau=5.0
num_ctx=9092, num_keep=5, repeat_last_n=128, repeat_penalty=1.2
temperature=0.2, seed=-1, tfs_z=1.0, num_predict=-1
top_k=40, top_p=0.8, min_p=0.5

==================== Generated Output ====================
{
    "dim": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.modules.mha.MHA.__init__] The dimension of the input embedding or hidden states in Transformer models.",
    "num_heads": "[flash_attn.modules.mha.MHA.__init__, tests.modules.test_block_parallel.test_block_parallel] Number of attention heads used for multi-head self-attention mechanism. This hyperparameter determines how many parallel attention mechanisms are applied to different parts (or projections) of the input embedding or hidden states.",
    "head_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality of each head in a multi-headed Transformer model, calculated as `embed_dim // num_heads` where embed_dim is the total size of the embeddings/hidden state vectors. This parameter determines how much information can be processed by individual attention heads.",
    "rotary_emb_dim": "[flash_attn.modules.mha.MHA.__init__] The dimensionality used for rotary position embedding, which helps in incorporating positional information into self-attention mechanisms without increasing computational complexity or memory usage significantly.",
    "seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence length of the input sequence. This hyperparameter determines how long each sequence is in terms of tokens and affects attention calculations over time steps for self-attention mechanisms.",
    "batch_size": "[flash_attn.modules.mha.MHA.allocate_inference_cache, tests.modules.test_block_parallel.test_block_parallel] The number of sequences processed simultaneously during training or inference. This parameter influences the batch size used when processing input data through Transformer models and affects parallelism in computation.",
    "max_seqlen": "[tests.modules.test_block_parallel.test_block_parallel, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Maximum sequence length that can be handled by the model or function. This hyperparameter is crucial for setting up memory requirements and computational limits when dealing with varying input lengths.",
    "softmax_scale": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Scaling factor applied to QK^T before applying softmax in the attention mechanism. This parameter helps stabilize numerical computations during training and inference by scaling down dot products between query and key vectors.",
    "window_size": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Size of sliding window for local (sliding) attention. This hyperparameter determines the range within which each token can attend to other tokens, affecting how far back or forward in a sequence an element's context is considered.",
    "cache_seqlens": "[flash_attn.modules.mha.MHA._apply_rotary_update_kvcache_attention, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Sequence lengths of the cached keys and values. This parameter helps manage memory usage during incremental decoding by specifying how much context to keep in cache.",
    "rotary_interleaved": "[flash_attn.modules.mha.MHA.__init__, flash_attn.flash_attn_interface.flash_attn_with_kvcache] Whether rotary embeddings are interleaved or not, affecting the way positional information is incorporated into self-attention mechanisms."
}

